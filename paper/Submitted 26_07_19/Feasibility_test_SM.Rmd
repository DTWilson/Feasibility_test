---
title: "A hypothesis test of feasibility for pilot trials assessing recruitment, follow-up and adherence rates"
author: "D. T. Wilson"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(ggplot2)
require(pso)
require(mco)
require(RColorBrewer)
require(rgenoud)
require(randtoolbox)
require(gganimate)
require(Rcpp)
cols <- brewer.pal(8, "Dark2")
```

## Introduction

This RMarkdown document contains the R code which generates the results and figures included in the manuscript of the same name, plus supplementary material.

## Example

### Hypotheses

We first find the threshold values $x_0, x_1$ which correspond to obtaining 65 and 80% power and set our mean difference, $\mu$.

```{r}
mu <- 0.3
p0 <- 0.65; p1 <- 0.8

x0 <- (qnorm(p0) + qnorm(0.975))
x1 <- (qnorm(p1) + qnorm(0.975))
```

These in turn define our hypotheses, $\hat{\Phi}_0$ and $\hat{\Phi}_1$. We can visualise the boudaries of these spaces:

```{r}
get_exp_N <- function(n_e, n_t, phi_r)
{
  # Calculate the expected number recruited into the main trial
  n_r <- 0:(n_t-1)
  sum(dbinom(n_r, n_e, phi_r)*n_r) + n_t*(1-pbinom(n_t-1, n_e, phi_r))
}

get_fup <- function(phi, xi, mu, n_e, n_t)
{
  # For some recruitment and adherence rate, find the follow-up rate which will lie on
  # the boundary of the hypothesis defined by xi
  p_r <- phi[1]; p_a <- phi[2]; sd <- 1
  exp_n <- get_exp_N(n_e, n_t, p_r)
  (4*sd^2 + 2*mu^2 *p_a*(1-p_a))*xi^2/((p_a*mu)^2*exp_n)

}

# Null
df <- expand.grid(p_r=seq(0,1,0.01), p_a=seq(0,1,0.01))
df$p_f <- apply(df, 1, get_fup, xi=x0, mu=mu, n_e = 1000, n_t =514)
sub <- df

# Alternative
df <- expand.grid(p_r=seq(0,1,0.01), p_a=seq(0,1,0.01))
df$p_f <- apply(df, 1, get_fup, xi=x1, mu=mu, n_e = 1000, n_t =514)
sub_a <- df

# Combine for plotting
sub$h <- "Null"
sub_a$h <- "Alternative"
sub <- rbind(sub, sub_a)
# Discard any infeasible points
sub <- sub[sub$p_f <= 1,]

p <- ggplot(sub, aes(p_r, p_a, z=p_f, colour=p_f)) +
  geom_contour(aes(colour=..level..), breaks=c(0.5,0.6,0.7,0.8,0.9,1)) +
  theme_minimal() + xlab("Recruitment") + ylab("Adherence") + 
  guides(colour = guide_legend(title = "Follow-up")) +
  facet_grid(. ~ factor(h, levels=c("Null", "Alternative"))) +
  theme(panel.spacing = unit(1, "lines"))
p

#ggsave("./paper/figures/hyps.pdf", height=9, width=14, units="cm")
#ggsave("./paper/figures/hyps.eps", height=9, width=14, units="cm", device = cairo_ps())
```

### Local error rates

Having defined our hypotheses, we can now calculate local type I and II error rates for a proposed pilot trial with sample size $n$, critical value $c$, and at two specific points in the null and alternative hypotheses.

```{r}
# Using a c++ implemntation of forumla (X)
Rcpp::sourceCpp('./src/comp_ocs.cpp')

get_ocs <- function(y, n, x0, x1, mu=0.3, n_e, n_t, exp_ns)
{
  # Input y = (c, [point in null space], point in alternative space])
  c <- y[1]; sd <- 1
  
  # Under perfect adherence and follow-up, will need an expected sample size of at least
  # 4c^2 / mu^2
  # to get a positive result
  exp_ns <- exp_ns[exp_ns >= (4*c^2)/(mu^2)]
  
  # Probability of +ve result at the null point
  p_r <- y[2]; p_a <- y[3]; p_f <- y[4]; 
  alpha <- get_comp_ocs_cpp5(matrix(c(c, p_r, p_a, p_f, sd), ncol=5), exp_ns, n, mu, n_e, n_t)

  # Probability of -ve result at the alternative point
  p_r <- y[5]; p_a <- y[6]; p_f <- y[7]; 
  beta <- 1 - get_comp_ocs_cpp5(matrix(c(c, p_r, p_a, p_f, sd), ncol=5), exp_ns, n, mu, n_e, n_t)

  # Returning negative values for optimisation later
  return(-c(alpha, beta))
}

n <- 50; n_e <- 1000; n_t <- 514

exp_ns <- NULL
for(s in 0:1000){
  r_est <- 2*n/(s+2*n)
  x <- 0
  for(k in 0:(n_t-1)){
    x <- x + dbinom(k, n_e, r_est)*k
  }
  exp_ns <- c(exp_ns, x + n_t*(1 - pbinom(n_t-1, n_e, r_est)))
}
# Under perfect adherence and follow-up, will need an expected sample size of at least
# 4c^2 / mu^2
# to get a positive result
exp_ns <- exp_ns[exp_ns >= (4*2.46^2)/(mu^2)]

# For example,
get_ocs(c(2.4, 0.64, 0.73, 0.8769883, 0.96, 1, 0.7422108), 50, x0, x1, 0.3, 1000, 514, exp_ns=exp_ns) 
```

### Global error rates

To solve the multi-objective optimisation problem described in the paper, we implment a constriant function to limit the search to our null and alternative hypotheses. The function returns a value for the null (alternative) point, being negative if the point is not within the null (alternative) hypothesis.

```{r}
get_constr <- function(y, n, x0, x1, mu=0.3, n_e, n_t, exp_ns)
{
  sd <- 1
  
  p_r <- y[2]; p_a <- y[3]; p_f <- y[4]; 
  exp_n <- get_exp_N(n_e, n_t, p_r)
  con_a <-  p_a*mu*sqrt(p_f*exp_n)/sqrt(4*sd^2 + 2*mu^2 * p_a*(1-p_a))

  p_r <- y[5]; p_a <- y[6]; p_f <- y[7]; 
  exp_n <- get_exp_N(n_e, n_t, p_r)
  con_b <-  p_a*mu*sqrt(p_f*exp_n)/sqrt(4*sd^2 + 2*mu^2 * p_a*(1-p_a))

  return(c(x0-con_a, con_b-x1))
}

# For example, change the alternative point from above so it is outside the alternative hypothesis
get_constr(c(2.4, 0.64, 0.73, 0.8769883, 0.96, 0.9, 0.7422108), 50, x0, x1, 0.3, 1000, 514)
```

We can now sole the problem using the NSGA-II algrothim from the `mco` package. Note that while there is an option to use a fully vectorised version (i.e. with the objective and constrained functions implemented in c++), this did not lead to any efficiency gain and so we have used R functions for greater transparancy.

```{r, eval = F}

ptm <- proc.time()

opt <- nsga2(fn=get_ocs, 7, 2, 
             n=50, x0=qnorm(0.65) + qnorm(0.975), x1=x1, mu=0.3, n_e=1000, n_t=514, exp_ns=exp_ns,
             lower.bounds = c(2, rep(0.1,6)), upper.bounds = c(3, rep(1,6)),
             generations = 1:20, popsize = 100,
             constraints = get_constr, cdim=2)

proc.time() - ptm

#   user  system elapsed 
# 595.55    0.75  602.27 

#saveRDS(opt, "./data/opt.Rda")
```

The algorithm returns a set of `popsize` solutions, each containing a critical value, a point in the null, and a point in the alternative. Each solution has a type I and II error, which we can plot to visualise the error rates available as we vary the critical value but keep the pilot sample size fixed.

```{r}
opt <- readRDS("./data/opt.Rda")

df <- data.frame(a=-opt[[200]]$value[,1], b=-opt[[200]]$value[,2])
df2 <- data.frame(a=-opt[[180]]$value[,1], b=-opt[[180]]$value[,2])

ggplot(df, aes(a, b)) + geom_point() + geom_line() +
  geom_line(data=df2, colour="red") +
  xlab(expression(alpha)) + ylab(expression(beta)) +
  theme_minimal() + 
  scale_y_continuous(breaks = seq(0,1,0.2)) + scale_x_continuous(breaks = seq(0,1,0.2)) +
  coord_fixed() #+ 
  # point evaluated with c=2.6 and OR=1
  #geom_point(data=data.frame(a=0.1285390, b=0.1496154), colour="green") +
  # point evaluated with c=2.6 and OR=5
  #geom_point(data=data.frame(a=0.1285390, b=0.3452193), colour="blue")
```


The function does not by default report any information which could be used to assess convergence. We can do so by extracting the solution set obtained at each iteration in the final portion of the search (say, the last quarter) and for each set measuring its corresponding dominated hypervolume. If the algorithm has converged, this measure should plateau towards the end of the search.

```{r}
DHs <- NULL
for(i in 1:length(opt)){
  dh <- dominatedHypervolume(-opt[[i]]$value, c(1,1))
  DHs <- c(DHs, dh)
}
plot(DHs)
```

### Alternative perspective

Rather than keeping the hypotheses fixed and looking for the error rates which can be obtained, we could instead constrain the type II error at some desired value and then look at how much type I error we will get for different hypotheses. Specifically, we can find the $c$ which will give a desired type II in the pilot for a fixed alternative defined by $p_1$, and then find the type I which results from a range of $p_0$.

```{r, eval=F}
n <- 70; n_e <- 1000; n_t <- 514

exp_ns <- NULL
for(s in 0:1000){
  r_est <- 2*n/(s+2*n)
  x <- 0
  for(k in 0:(n_t-1)){
    x <- x + dbinom(k, n_e, r_est)*k
  }
  exp_ns <- c(exp_ns, x + n_t*(1 - pbinom(n_t-1, n_e, r_est)))
}
# Under perfect adherence and follow-up, will need an expected sample size of at least
# 4c^2 / mu^2
# to get a positive result
exp_ns <- exp_ns[exp_ns >= (4*2.46^2)/(mu^2)]

# Manually find a c that gives approx 90% pilot power,
# for n_p = 30, 50, 70
psoptim(rep(NA,2), fn=get_oc2_opt,
        upper = c(1,1), lower = c(0.1,0.1),
                 c=2.57, n=70, x=x1, a=F, mu=0.3, n_e=1000, n_t=514, exp_ns=exp_ns,
        control=list(trace=1, reltol = 0.01, maxit = 200))
cs <- c(2.46, 2.53, 2.57)

MO_alt <- function(pars, c, n, a, mu, n_e, n_t, exp_ns)
{ 
  x0 <- qnorm(pars[1]) + qnorm(0.975)
  c(pars[1], get_oc2_opt(pars[2:4], c=c, n=n_p, x=x0, a=a, mu=0.3, n_e=1000, n_t=514, exp_ns=exp_ns))
}

# For a range of p_0 and n_p, find the type I error
df <- NULL
for(n_p in c(30, 50, 70)){
      print(n_p)
  
    c <- cs[which(c(30, 50, 70) == n_p)]
    
    exp_ns <- NULL
    for(s in 0:1000){
      r_est <- 2*n_p/(s+2*n_p)
      x <- 0
      for(k in 0:(n_t-1)){
        x <- x + dbinom(k, n_e, r_est)*k
      }
      exp_ns <- c(exp_ns, x + n_t*(1 - pbinom(n_t-1, n_e, r_est)))
    }
    exp_ns <- exp_ns[exp_ns >= (4*c^2)/(mu^2)]

    opt <- nsga2(MO_alt, 3,2,lower.bounds = c(0.45,0.1,0.1), upper.bounds = c(0.8, 1,1),
                 generations = 500, popsize = 200,
             c=c, n=n_p, a=T, mu=0.3, n_e=1000, n_t=514, exp_ns=exp_ns)
    
    df <- rbind(df, data.frame(p0 = opt$value[,1], a =-opt$value[,2], n_p=n_p))
}

plot(df[,1:2])

saveRDS(df, "./data/alt_data.Rda")
```

```{r}
df <- readRDS("./data/alt_data.Rda")

ggplot(df, aes(p0, a, colour=as.factor(n_p))) + geom_line() + #geom_smooth(span=0.4) +
  scale_color_manual(name=expression(n[p]), values=cols) +
  xlab(expression(p[0])) + ylab(expression(alpha)) +
  theme_minimal()

#ggsave("./paper/figures/alt.pdf", height=9, width=14, units="cm")
#ggsave("./paper/figures/alt.eps", height=9, width=14, units="cm", device = cairo_ps())
```

### Comparator

We can comapre the error rate curve of the proposed method against that of the current approach, where the three parameter estimates are compared against seperate progression criteria thresholds and a "go" decision is made only if they are all exceeded. The optimisation strategy is based on a simple grid search at the inner (over the hypotheses) level. 

```{r}
get_pf <- function(phi, xi, mu, n_e, n_t)
{
  p_r <- phi[1]; p_a <- phi[2]; sd <- 1
  if(any(c(p_r,p_a) < 0) | any(c(p_r,p_a) > 1) ){return(1000)}
  exp_n <- get_exp_N(n_e, n_t, p_r)
  (4*sd^2 + 2*mu^2 *p_a*(1-p_a))*xi^2/((p_a*mu)^2*exp_n)
}

get_oc_pc <- function(cs, n, m, tI)
{
  # Recruitment
  s <- floor(2*n/cs[1] - 2*n)
  # Adherence
  a <- cs[2]*n
  # Follow-up
  f <- cs[3]*2*n
  
  (tI*-1 + !tI*1)*pnbinom(s, 2*n, m[,1])*(1- pbinom(f, 2*n, m[,3]))*(1 - pbinom(a, n, m[,2]))
}

# for example,

m0 <- expand.grid(p_r = seq(0,1,0.005), p_a= seq(0,1,0.005))
m0$p_f <- apply(m0, 1, get_pf, xi=x0, mu=0.3, n_e=1000, n_t=514)
m0 <- m0[m0$p_f <= 1,]

m1 <- expand.grid(p_r = seq(0,1,0.005), p_a= seq(0,1,0.005))
m1$p_f <- apply(m1, 1, get_pf, xi=x1, mu=0.3, n_e=1000, n_t=514)
m1 <- m1[m1$p_f <= 1,]

cs <- c(0.3, 0.8, 0.7)

alphas <- get_oc_pc(cs, n=50, m=m0, tI=T)
betas <- get_oc_pc(cs, n=50, m=m1, tI=F)

c(-min(alphas), 1-min(betas))
```

We can search over all possible progression criteria and plot the best available error rates using nsga-II:
```{r, eval=F}
MO_pc <- function(cs, n, m0, m1)
{
  alphas <- -get_oc_pc(cs, n=n, m=m0, tI=T)
  pows <- get_oc_pc(cs, n=n, m=m1, tI=F)

  c(max(alphas), 1-min(pows))
}

opt <- nsga2(MO_pc, 3, 2, 
             lower.bounds = rep(0,3), upper.bounds = rep(1,3),
             n=30, m0=m0, m1=m1)

df <- data.frame(a=opt$value[,1], b=opt$value[,2])

ggplot(df, aes(a,b)) + geom_point() + geom_step() +
  theme_minimal() + coord_fixed()
```

## Evaluation

We find the OC curves produced by the proposed method and for the comparator for a range of null hypothesis power parameters and a range of total sample sizes of the main trial. FOr each scenario, we consider pilot sample sizes of n = 30, 50 and 70.

```{r, eval = F}
eval_scenario <- function(null_pow, n_t, n)
{
  print(n)
  p0 <- null_pow; p1 <- 0.8
  n_e <- 1000

  x0 <- qnorm(p0) + qnorm(0.975)
  x1 <- qnorm(p1) + qnorm(0.975)
  
  exp_ns <- NULL
  for(s in 0:1000){
    r_est <- 2*n/(s+2*n)
    x <- 0
    for(k in 0:(n_t-1)){
      x <- x + dbinom(k, n_e, r_est)*k
    }
    exp_ns <- c(exp_ns, x + n_t*(1 - pbinom(n_t-1, n_e, r_est)))
  }
  
  opt <- nsga2(fn=get_ocs, 7, 2, 
             n=n, x0=x0, x1=x1, mu=0.3, n_e=1000, n_t=n_t, exp_ns=exp_ns,
             lower.bounds = c(2, rep(0.1,6)), upper.bounds = c(3, rep(1,6)),
             generations = 500, popsize = 300,
             constraints = get_constr, cdim=2)
  
  df <- cbind(opt$par[,1], -opt$value)
  
  #df <- t(sapply(seq(2,3,0.05), get_ocs2, n=n, x0=x0, x1=x1, mu=0.3, n_e=1000, n_t=n_t))
  #df <- cbind(seq(2,3,0.05), df[,1:2])
  
  m0 <- expand.grid(p_r = seq(0,1,0.005), p_a= seq(0,1,0.005))
  m0$p_f <- apply(m0, 1, get_pf, xi=x0, mu=0.3, n_e=1000, n_t=n_t)
  m0 <- m0[m0$p_f <= 1,]
  
  m1 <- expand.grid(p_r = seq(0,1,0.005), p_a= seq(0,1,0.005))
  m1$p_f <- apply(m1, 1, get_pf, xi=x1, mu=0.3, n_e=1000, n_t=n_t)
  m1 <- m1[m1$p_f <= 1,]
  
  opt_comp <- nsga2(MO_pc, 3, 2, 
             lower.bounds = rep(0,3), upper.bounds = rep(1,3),
             popsize = 12, generations = 2,
             n=n, m0=m0, m1=m1)
  
  df2 <- data.frame(a=c(df[,2], opt_comp$value[,1]),
                   b=c(df[,3], opt_comp$value[,2]),
                   t=c(rep("FT", nrow(df)), rep("PC", nrow(opt_comp$value))))
  df2$n <- n; df2$n_t <- n_t; df2$null_pow <- null_pow
  
  df2 <- cbind(df2, rbind(cbind(df[,1], rep(0,nrow(df)), rep(0,nrow(df))), opt_comp$par))
  names(df2)[7:9] <- c("c_1", "c_2", "c_3")
  
  return(df2)
}

df <- NULL
for(i in c(468, 514, 562)){
  for(j in c(0.6, 0.65, 0.7)){
    for(k in c(30, 50, 70)){
        df <- rbind(df, eval_scenario(j, i, k))
    }
  }
}

names(df)[4] <- "np"

df$null_pow <- factor(df$null_pow, levels=c(0.6, 0.65, 0.7),
                         labels=c("p[0] == 0.6", "p[0] == 0.65", "p[0] == 0.7"))
df$n_t <- factor(df$n_t, levels=c(468, 514, 562),
                 labels=c("n[t] == 468", "n[t] == 514", "n[t] == 562"))

#saveRDS(df, "./data/eval.Rda")
```

Plot the results

```{r}
df <- readRDS("./data/eval.Rda")

ggplot(df, aes(a, b, colour=as.factor(np), linetype=t)) + geom_step() + 
  facet_grid(n_t ~ null_pow, labeller = label_parsed) +
  theme_minimal() + coord_fixed() +
  xlab(expression(alpha)) + ylab(expression(beta)) +
  scale_color_manual(name=expression(n[p]), values=cols) +
  scale_linetype(name="Method", labels=c("Feasibility test", "Standard practice")) +
  theme(panel.spacing = unit(2, "lines"), legend.position="bottom") +
  scale_x_continuous(breaks=c(0, 0.25, 0.5, 0.75, 1)) 

#ggsave("./paper/figures/eval.pdf", height=18, width=20, units="cm")
#ggsave("./paper/figures/eval.eps", height=18, width=20, units="cm", device = cairo_ps())
```

Pick out some specific numbers for comparison.
```{r}
df[df$n_t == "n[t] == 514" & df$a > 0.08 & df$a < 0.12 & df$t == "FT" & df$np == 50,]
```

Look at one scenario in more detail.
```{r}
sub <- df[df$null_pow == "p[0] == 0.65" & df$n_t == "n[t] == 514",]

ggplot(sub, aes(a,b,colour=as.factor(np), linetype=t)) + geom_step() + #geom_point() +
  theme_minimal() + coord_fixed() +
  xlab(expression(alpha)) + ylab(expression(beta)) +
  scale_color_manual(name=expression(n[p]), values=cols) +
  scale_linetype(name="Method", labels=c("Feasibility test", "Standard practice")) +
  scale_x_continuous(breaks=c(0, 0.2, 0.4, 0.6, 0.8, 1)) + 
  scale_y_continuous(breaks=c(0, 0.2, 0.4, 0.6, 0.8, 1)) 

#ggsave("./paper/figures/ex_ocs.pdf", height=9, width=14, units="cm")
#ggsave("./paper/figures/ex_ocs.eps", height=9, width=14, units="cm", device = cairo_ps())
```

We can relax the assumption of independance between the follow-up and adherence outcomes.

```{r, eval=F}
get_oc_cor <- function(phi, n, cc, xi, a=TRUE, mu=0.3, n_e, n_t, or)
{
  p_r <- phi[1]; p_a <- phi[2]; sd <- 1
  if(any(c(p_r,p_a) < 0) | any(c(p_r,p_a) > 1) ){return(1000)}
  exp_n <- get_exp_N(n_e, n_t, p_r)
  p_f <- (2*sd^2 + mu^2 *p_a*(1-p_a))*xi^2/((p_a*mu)^2*exp_n)
  if(any(c(p_r,p_a,p_f) < 0) | any(c(p_r,p_a,p_f) > 1) ){
    1000
  } else {
    (a*-1 + !a*1)*get_comp_ocs_cpp4(cc, n, matrix(c(p_r, p_a, p_f, or), ncol=4), mu, n_e, n_t)
  }
}

get_ocs_cor <- function(c, n, x0, x1, mu=0.3, n_e, n_t, or)
{
  opt_a <- optim(c(0.9,0.9), fn=get_oc_cor,
                 cc=c, n=n, xi=x0, a=T, mu=mu, n_e=n_e, n_t=n_t, or=1)
  
  opt_b <- optim(c(0.9,0.9), fn=get_oc_cor,
                 cc=c, n=n, xi=x1, a=F, mu=mu, n_e=n_e, n_t=n_t, or=or)
  
  c(-opt_a$value, 1-opt_b$value)
}

get_ocs_cor(2.6, n=50, x0=qnorm(0.65) + qnorm(0.975), x1, mu=0.3, n_e=1000, n_t=514, or=5)
```

## Extensions

### Estimating outcome SD

We have a c++ function implementing the relevant equation. We can check it against a simulation:

```{r}
# Compare with simulation
sim <- function(phi, n, c, xi, mu, n_e, n_t)
{
  p_r <- phi[1]; p_a <- phi[2]; p_f <- phi[3] 
  exp_n <- get_exp_N(n_e, n_t, p_r)
  sd <- sqrt( (p_f*(p_a*mu)^2*exp_n - 2*mu^2 *p_a*(1-p_a)*xi^2)/(4*xi^2) )
  
  a_est <- rbinom(1, n, p_a)/n
  f <- rbinom(1, 2*n, p_f)
  f_est <- f/(2*n)
  r_est <- 2*n/(rnbinom(1, 2*n, p_r) + 2*n)
  var_est <- rchisq(1, f-1)*(sd^2)/(f-1)
  
  stat <- mu*a_est*sqrt(get_exp_N(n_e, n_t, r_est)*f_est)/sqrt((4*var_est + 2*mu*mu*a_est*(1-a_est)))
  stat > c
}
  
phi <- c(0.5196304, 0.6670084, 0.9997755); n_e <- 1000; n_t <- 514; xi <- x1; mu <- 0.3
p_r <- phi[1]; p_a <- phi[2]; p_f <- phi[3] 
exp_n <- get_exp_N(n_e, n_t, p_r)
sd <- sqrt( (p_f*(p_a*mu)^2*exp_n - 2*mu^2 *p_a*(1-p_a)*xi^2)/(4*xi^2) )

get_ocs_var_cpp(c=2.4, n=30, matrix(c(phi, sd), ncol=4), mu=0.3, n_e=1000, n_t=514)
mean(replicate(10000, sim(phi, n=30, c=2.4, xi=x1, mu=0.3, n_e=1000, n_t=514)))
```

Now we can find error rates under the same multi-objective optimisation framework as before:

```{r}
get_ocs_var <- function(y, n, x0, x1, mu=0.3, n_e, n_t)
{
  # Input y = (c, [point in null space], point in alternative space])
  c <- y[1]
  
  # Probability of +ve result at the null point
  p_r <- y[2]; p_a <- y[3]; p_f <- y[4]; sd <- y[5]
  alpha <- get_ocs_var_cpp(c=c, n=n, matrix(c(p_r, p_a, p_f, sd), ncol=4), mu, n_e, n_t)

  # Probability of -ve result at the alternative point
  p_r <- y[6]; p_a <- y[7]; p_f <- y[8]; sd <- y[9]
  beta <- 1 - get_ocs_var_cpp(c=c, n=n, matrix(c(p_r, p_a, p_f, sd), ncol=4), mu, n_e, n_t)

  # Returning negative values for optimisation later
  return(-c(alpha, beta))
}

# For example,
get_ocs_var(c(2.4, 0.64, 0.73, 0.8769883, 1, 0.96, 1, 0.7422108, 0.9), 30, x0, x1, 0.3, 1000, 514) 
```

```{r}
get_constr_var <- function(y, n, x0, x1, mu=0.3, n_e, n_t)
{
  p_r <- y[2]; p_a <- y[3]; p_f <- y[4]; sd <- y[5]
  exp_n <- get_exp_N(n_e, n_t, p_r)
  con_a <-  p_a*mu*sqrt(p_f*exp_n)/sqrt(4*sd^2 + 2*mu^2 * p_a*(1-p_a))

  p_r <- y[6]; p_a <- y[7]; p_f <- y[8]; sd <- y[9]
  exp_n <- get_exp_N(n_e, n_t, p_r)
  con_b <-  p_a*mu*sqrt(p_f*exp_n)/sqrt(4*sd^2 + 2*mu^2 * p_a*(1-p_a))

  return(c(x0-con_a, con_b-x1))
}

# For example, change the alternative point from above so it is outside the alternative hypothesis
get_constr_var(c(2.4, 0.64, 0.73, 0.8769883, 1, 0.96, 0.9, 0.7422108, 0.95), 30, x0, x1, 0.3, 1000, 514)
```

Now optimise, but with the search constrained to $0.8 < \sigma$:

```{r, eval = F}
ptm <- proc.time()

opt <- nsga2(fn=get_ocs_var, 9, 2, 
             n=70, x0=qnorm(0.65) + qnorm(0.975), x1=x1, mu=0.3, n_e=1000, n_t=514,
             lower.bounds = c(2, 0.1,0.1,0.1, 0.8,  0.1,0.1,0.1, 0.8),
             upper.bounds = c(3, 1,1,1, 2, 1,1,1, 2),
             generations = 1:200, popsize = 100,
             constraints = get_constr_var, cdim=2)

proc.time() - ptm

#   user  system elapsed 
# 3916.90    5.93 3963.75 , so ~ 1.1 hours 

#saveRDS(opt, "./data/opt_var.Rda")
#saveRDS(opt, "./data/opt_var_n70.Rda")
```

Check convergence:

```{r}
DHs <- NULL
for(i in 1:length(opt)){
  dh <- dominatedHypervolume(-opt[[i]]$value, c(1,1))
  DHs <- c(DHs, dh)
}
plot(DHs)
```
Compare with the same scenario but where we didn't include sd estimation:

```{r}
opt <- readRDS("./data/opt.Rda")
opt_var <- readRDS("./data/opt_var.Rda")
#opt_var <- readRDS("./data/opt_var_n70.Rda")

df <- data.frame(a=-opt[[200]]$value[,1], b=-opt[[200]]$value[,2])
df_var <- data.frame(a=-opt_var[[200]]$value[,1], b=-opt_var[[200]]$value[,2])

df <- rbind(df, df_var)
df$t <- c(rep("Known variance", 100), rep("Unknown variance", 100))

ggplot(df, aes(a, b, shape=t, linetype=t)) + 
  geom_point() +
  geom_line() +
  scale_shape_manual(values=c(16, 2)) +
  xlab(expression(alpha)) + ylab(expression(beta)) +
  labs(shape = "", linetype="") +
  theme_minimal() +
  scale_y_continuous(breaks = seq(0,1,0.2)) + scale_x_continuous(breaks = seq(0,1,0.2)) +
  coord_fixed()

#ggsave("./paper/figures/var_ocs.pdf", height=9, width=14, units="cm")
#ggsave("./paper/figures/var_ocs.eps", height=9, width=14, units="cm", device = cairo_ps())
```

## References




