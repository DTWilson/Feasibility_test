---
title: "Testing feasibility - recruitment, adherence and data collection"
author: "D. T. Wilson"
date: "16 August 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(ggplot2)
require(pso)
require(mco)
require(RColorBrewer)
require(rgenoud)
colours <- brewer.pal(8, "Dark2")
```

## Introduction

## Hypotheses

We first find the threshold values $x_0, x_1$ which correspond to obtaining 60 and 80% power and set our mean difference, $\mu$.

```{r}
mu <- 0.3
p0 <- 0.6; p1 <- 0.8

x0 <- (qnorm(p0) + qnorm(0.975))
x1 <- (qnorm(p1) + qnorm(0.975))
```

These in turn define our hypotheses, $\hat{\Phi}_0$ and $\hat{\Phi}_1$. We can visualise the boudaries of these spaces:

```{r}
get_exp_N <- function(n_e, n_t, phi_r)
{
  # Calculate the expected number recruited into the main trial
  n_r <- 0:(n_t-1)
  sum(dbinom(n_r, n_e, phi_r)*n_r) + n_t*(1-pbinom(n_t-1, n_e, phi_r))
}

get_fup <- function(phi, xi, mu, n_e, n_t)
{
  # For some recruitment and adherence rate, find the follow-up rate which will lie on
  # the boundary of the hypothesis defined by xi
  p_r <- phi[1]; p_a <- phi[2]; sd <- 1
  exp_n <- get_exp_N(n_e, n_t, p_r)
  (2*sd^2 + mu^2 *p_a*(1-p_a))*xi^2/((p_a*mu)^2*exp_n)

}

# Null
df <- expand.grid(p_r=seq(0,1,0.01), p_a=seq(0,1,0.01))
df$p_f <- apply(df, 1, get_fup, xi=x0, mu=mu, n_e = 500, n_t =235)
sub <- df

# Alternative
df <- expand.grid(p_r=seq(0,1,0.01), p_a=seq(0,1,0.01))
df$p_f <- apply(df, 1, get_fup, xi=x1, mu=mu, n_e = 500, n_t =235)
sub_a <- df

# Combine for plotting
sub$h <- "Null"
sub_a$h <- "Alternative"
sub <- rbind(sub, sub_a)
# Discard any infeasible points
sub <- sub[sub$p_f <= 1,]

ggplot(sub, aes(p_r, p_a, z=p_f, colour=p_f)) +
  geom_contour(aes(colour=..level..)) +
  theme_minimal() + xlab("Recruitment") + ylab("Adherence") + 
  guides(colour = guide_legend(title = "Follow-up")) +
  facet_grid(. ~ factor(h, levels=c("Null", "Alternative"))) +
  theme(panel.spacing = unit(1, "lines"))

#ggsave("./paper/figures/hyps.pdf", height=9, width=14, units="cm")
#ggsave("./paper/figures/hyps.eps", height=9, width=14, units="cm", device = cairo_ps())
```

## Local error rates

Having defined our hypotheses, we can now calculate local type I and II error rates for a proposed pilot trial with sample size $n$, critical value $c$, and at two specific points in the null and alternative hypotheses.

```{r}
# Using a c++ implemntation of forumla (X)
Rcpp::sourceCpp('./src/comp_ocs.cpp')

get_ocs <- function(y, n, x0, x1, mu=0.3, n_e, n_t)
{
  # Input y = (c, [point in null space], point in alternative space])
  c <- y[1]; sd <- 1
  
  # Probability of +ve result at the null point
  p_r <- y[2]; p_a <- y[3]; p_f <- y[4]; 
  alpha <- get_comp_ocs_cpp3(matrix(c(c, p_r, p_a, p_f, sd), ncol=5), n, mu, n_e, n_t)

  # Probability of -ve result at the alternative point
  p_r <- y[5]; p_a <- y[6]; p_f <- y[7]; 
  beta <- 1 - get_comp_ocs_cpp3(matrix(c(c, p_r, p_a, p_f, sd), ncol=5), n, mu, n_e, n_t)

  # Returning negative values for optimisation later
  return(-c(alpha, beta))
}

# For example,
get_ocs(c(2.4, 0.64, 0.73, 0.8769883, 0.96, 1, 0.7422108), 30, x0, x1, 0.3, 500, 235)
```

## Global error rate curves

To solve the multi-objective optimisation problem described in the paper, we need to implment a constriant function to limit the search to our null and alternative hypotheses. The function returns a value for the null (alternative) point, being negative if the point is not within the null (alternative) hypothesis.

```{r}
get_constr <- function(y, n, x0, x1, mu=0.3, n_e, n_t)
{
  sd <- 1
  
  p_r <- y[2]; p_a <- y[3]; p_f <- y[4]; 
  exp_n <- get_exp_N(n_e, n_t, p_r)
  con_a <-  p_a*mu*sqrt(p_f*exp_n)/sqrt(2*sd^2 + mu^2 * p_a*(1-p_a))

  p_r <- y[5]; p_a <- y[6]; p_f <- y[7]; 
  exp_n <- get_exp_N(n_e, n_t, p_r)
  con_b <-  p_a*mu*sqrt(p_f*exp_n)/sqrt(2*sd^2 + mu^2 * p_a*(1-p_a))

  return(c(x0-con_a, con_b-x1))
}

# For example, change the alternative point from above so it is outside the alternative hypothesis
get_constr(c(2.4, 0.64, 0.73, 0.8769883, 0.96, 0.9, 0.7422108), 30, x0, x1, 0.3, 500, 235)
```

We can now sole the problem using the NSGA-II algrothim from the "mco" package. Note that while these is an option to use a fully vectorised version (i.e. with the objective and constrained functions implemented in c++), this did not lead to any efficiency gain and so we have used R functions for greater transparancy.

```{r}
opt <- nsga2(fn=get_ocs, 7, 2, 
             n=30, x0=x0, x1=x1, mu=0.3, n_e=500, n_t=234,
             lower.bounds = c(2, rep(0.1,6)), upper.bounds = c(3, rep(1,6)),
             generations = 1:100, popsize = 100,
             constraints = get_constr, cdim=2)
```

The algorithm returns a set of `popsize` solutions, each containing a critical value, a point in the null, and a point in the alternative. Each solution has a type I and II error, which we can plot to visualise the error rates available as we vary the critical value but keep the pilot sample size fixed.

```{r}
df <- data.frame(a=-opt[[100]]$value[,1], b=-opt[[100]]$value[,2])
df2 <- data.frame(a=-opt[[80]]$value[,1], b=-opt[[80]]$value[,2])

ggplot(df, aes(a, b)) + geom_point() + geom_line() +
  geom_line(data=df2, colour="red") +
  xlab(expression(alpha)) + ylab(expression(beta)) +
  theme_minimal()
```

The function does not by default report any information which could be used to assess convergence. We can do so by extracting the solution set obtained at each iteration in the final portion of the search (say, the last quarter) and for each set measuring its corresponding dominated hypervolume. If the algorithm has converged, this measure should platue towards the end of the search.

```{r}
DHs <- NULL
for(i in 1:length(opt)){
  dh <- dominatedHypervolume(-opt[[i]]$value, c(1,1))
  DHs <- c(DHs, dh)
}
plot(DHs)
```

## Comparison

We can comapre the error rate curve of the proposed method against that of the current approach, where the three parameter estimates are compared against seperate progression criteria thresholds and a "go" decision is made only if they are all exceeded.

```{r}
get_local_oc_pc <- function(c_r, c_a, c_f, p_r, p_a, p_f, n)
{
  # Get threshold values corresponding to the PCs
  s <- floor(2*n/c_r - 2*n)
  a <- c_a*n
  f <- c_f*2*n
  
  # Get probability of all thresholds being exceeded
  pnbinom(s, 2*n, p_r)*(1- pbinom(f, 2*n, p_f))*(1 - pbinom(a, n, p_a))
}

get_ocs_pc <- function(y, n, x0, x1, mu=0.3, n_e, n_t)
{
  # Input y = (c, [point in null space], point in alternative space])
  c_r <- y[1]; c_a <- y[2]; c_f <- y[3]; sd <- 1
  
  # Probability of +ve result at the null point
  p_r <- y[4]; p_a <- y[5]; p_f <- y[6]; 
  alpha <- get_local_oc_pc(c_r, c_a, c_f, p_r, p_a, p_f, n)

  # Probability of -ve result at the alternative point
  p_r <- y[7]; p_a <- y[8]; p_f <- y[9]; 
  beta <- 1 - get_local_oc_pc(c_r, c_a, c_f, p_r, p_a, p_f, n)

  # Returning negative values for optimisation later
  return(-c(alpha, beta))
}

get_constr_pc <- function(y, n, x0, x1, mu=0.3, n_e, n_t)
{
  get_constr(y[3:9], n, x0, x1, mu=0.3, n_e, n_t)
}

opt <- nsga2(get_ocs_pc, 9, 2, n=50, x0=x0, x1=x1, mu=0.3, n_e=500, n_t=235,
             lower.bounds = c(0, 0, 0, rep(0.1, 6)), upper.bounds = rep(1,9),
              popsize = 20, generations = 50,
              constraints = get_constr_pc, cdim=2)
plot(opt$value)





get_oc_pc <- function(phi, cs, n, xi, tI, s, f, a, n_e, n_t)
{
  p_r <- phi[1]; p_a <- phi[2]; sd <- 1
  exp_n <- get_exp_N(n_e, n_t, p_r)
  p_f <- (2*sd^2 + mu^2 *p_a*(1-p_a))*xi^2/((p_a*mu)^2*exp_n)
  if(p_f <= 1){
    (tI*-1 + !tI*1)*pnbinom(s, 2*n, p_r)*(1- pbinom(f, 2*n, p_f))*(1 - pbinom(a, n, p_a)) 
  } else {
    1 + 1000*(p_f-1)^2
  }
}

get_ocs_pc <- function(cs, n, x0, x1, n_e, n_t)
{
  # df <- as.data.frame(sobol(1000,2)); names(df) <- c("p_r", "p_a")
  # df$p_f <- apply(df, 1, get_fup, xi=x0, mu=mu, n_e=n_e, n_t=n_t)
  # sub_n <- df[df$p_f <= 1,]
  # 
  # df <- as.data.frame(sobol(1000,2)); names(df) <- c("p_r", "p_a")
  # df$p_f <- apply(df, 1, get_fup, xi=x1, mu=mu, n_e=n_e, n_t=n_t)
  # sub_a <- df[df$p_f <= 1,]
  # 
  # # First, get the numbers corresponding to the 
  # # critical values and their respective probabilities
  # 
  #   # Recruitment
     s <- floor(2*n/cs[1] - 2*n)
  #   sub_n$a <- pnbinom(s, 2*n, sub_n[,1])
  #   # Adherence
     a <- cs[2]*n
  #   sub_n$a <- sub_n$a*(1 - pbinom(a, n, sub_n[,2]))
  #   # Follow-up
     f <- cs[3]*2*n
  #   sub_n$a <- sub_n$a*(1- pbinom(f, 2*n, sub_n[,3]))
  # 
  # 
  #   # Recruitment
  #   sub_a$a <- pnbinom(s, 2*n, sub_a[,1])
  #   # Adherence
  #   sub_a$a <- sub_a$a*(1 - pbinom(a, n, sub_a[,2]))
  #   # Follow-up
  #   sub_a$a <- sub_a$a*(1- pbinom(f, 2*n, sub_a[,3]))

    
    # Locally optimise using the worst error rate from the above as the start point
    
    opt_a <- optim(c(0.9,0.9), fn=get_oc_pc, lower = c(0.1,0.1), upper=c(1,1),
                 cs=cs, n=n, xi=x0, tI=T, s=s, f=f, a=a, n_e=n_e, n_t=n_t, method="L-BFGS-B")
    
    opt_b <- optim(c(0.9,0.9), fn=get_oc_pc, lower = c(0.1,0.1), upper=c(1,1),
                 cs=cs, n=n, xi=x1, tI=F, s=s, f=f, a=a, n_e=n_e, n_t=n_t, method="L-BFGS-B")
    
  
  c(-opt_a$value, 1-opt_b$value)
}

# For example,
get_ocs_pc(rep(0.5, 3), 30, x0, x1, n_e=500, n_t=235)
```
We can search over all possible progression criteria and plot the best available error rates.

```{r, eval=F}
opt <- nsga2(get_ocs_pc, 3, 2, n=30, x0=x0, x1=x1, n_e=500, n_t=235,
             lower.bounds = c(0, 0, 0), upper.bounds = c(1, 1, 1),
              popsize = 100, generations = 100)
plot(opt$value)

to_eval <- sobol(10000,3)
ocs <- apply(to_eval, 1, get_ocs_pc, n=30, x0=x0, x1=x1, n_e=500, n_t=235)
plot(t(ocs))

ocs <- t(ocs)
ocs2 <- paretoFilter(ocs)
ocs2 <- cbind(to_eval[ocs[,1] %in% ocs2[,1],], ocs2)

ocs2 <- as.data.frame(ocs2)
names(ocs2) <- c("c1", "c2", "c3", "a", "b")

ggplot(ocs2, aes(a,b)) + geom_point() + geom_step()

#saveRDS(ocs, "ex1_ocs_n30_pc.Rda")
#saveRDS(ocs, "ex1_ocs_n50_pc.Rda")
```


## Evaluation

We find the OC curves produced by the proposed method and for the comparator for a range of null hypothesis power parameters and a range of total sample sizes of the main trial. FOr each scenario, we consider pilot sample sizes of n = 30, 50 and 100.

```{r}
eval_scenario <- function(null_pow, n_t, n)
{
  print(n)
  p0 <- null_pow; p1 <- 0.8

  x0 <- qnorm(p0) + qnorm(0.975)
  x1 <- qnorm(p1) + qnorm(0.975)
  
  opt <- nsga2(fn=get_ocs, 7, 2, 
             n=n, x0=x0, x1=x1, mu=0.3, n_e=500, n_t=n_t,
             lower.bounds = c(2, rep(0.1,6)), upper.bounds = c(3, rep(1,6)),
             generations = 100, popsize = 100,
             constraints = get_constr, cdim=2)
  
  opt_comp <- nsga2(get_ocs_pc, 3, 2, n=n, x0=x0, x1=x1, n_e=500, n_t=n_t,
             lower.bounds = c(0, 0, 0), upper.bounds = c(1, 1, 1),
              popsize = 100, generations = 100)
  
  df <- data.frame(a=c(opt$value[,1], opt_comp$value[,1]),
                   b=c(opt$value[,2], opt_comp$value[,2]),
                   t=c(rep("FT", 100), rep("PC", 100)))
  df$n <- n; df$n_t <- n_t; df$null_pow <- null_pow
  
  return(df)
}

df <- NULL
for(i in c(234, 234*1.1, 234*1.2)){
  for(j in c(0.6, 0.65, 0.7)){
    for(k in c(30, 50, 100)){
          df <- rbind(df, eval_scenario(j, i, k))
    }
  }
}

saveRDS(df, "./data/eval.Rda")
```

## Example

## Extensions

### Correlated follow-up and adherence

### Unknown variance




```{r}
# Superceeded
get_oc <- function(phi, n, cc, xi, a=TRUE, mu=0.3, n_e, n_t, M)
{
  p_r <- phi[1]; p_a <- phi[2]; p_f <- phi[3]; sd <- 1
  exp_n <- get_exp_N(n_e, n_t, p_r)
  con <-  p_a*mu*sqrt(p_f*exp_n)/sqrt(2*sd^2 + mu^2 * p_a*(1-p_a))
  (a*-1 + !a*1)*get_comp_ocs_cpp3(cc, n, matrix(c(p_r, p_a, p_f, sd), ncol=4), mu, n_e, n_t) + 
  M*(con-xi)^2
  #a*M*(max(0, con-xi)^2) + (!a)*M*(max(0, xi-con)^2) 
  
  #sd <- sqrt(p_f*(p_a*mu)^2*exp_n/(2*xi^2) - mu^2 *p_a*(1-p_a)/2)
  #if(!is.na(sd)){
  #  (a*-1 + !a*1)*get_comp_ocs_cpp3(cc, n, matrix(c(p_r, p_a, p_f, sd), ncol=4), mu, n_e, n_t) + M*(1-sd)^2
  #} else {
  #  10000
  #}
} 

get_ocs <- function(c, n, x0, x1, mu, n_e, n_t, Ms, des_a=rep(0.1,3), des_b=rep(0.1,3), print.level=0)
{
  
  # Type I
  for(m in Ms){
    opt_a <- optim(des_a, fn=get_oc, lower = c(0.1,0.1,0.1), upper=c(1,1,1),
                  cc=c, n=n, xi=x0, a=T, mu=mu, n_e=n_e, n_t=n_t, method="L-BFGS-B", M=m)
    if(print.level == 1) print(opt_a$par)
    des_a <- opt_a$par
  }

  # Type II
  for(m in Ms){
    opt_b <- optim(des_b, fn=get_oc, lower = c(0.1,0.1,0.1), upper=c(1,1,1),
                  cc=c, n=n, xi=x1, a=F, mu=mu, n_e=n_e, n_t=n_t, method="L-BFGS-B", M=m)
    if(print.level == 1) print(opt_b$par)
    des_b <- opt_b$par
  }
  
  c(-opt_a$value, 1-opt_b$value, opt_a$par, opt_b$par)
}

get_ocs(c=2.6, n=30, x0=x0, x1=x1, mu=0.3, n_e=500, n_t=235, Ms = 10^seq(0:5), print.level = 1)

# Check against simulation
do_sim <- function(c, phi)
{
  mu <- 0.3; n <- 30; n_e <- 500; n_t <- 235
  p_r <- phi[1]; p_a <- phi[2]; p_f <- phi[3] 
  exp_n <- get_exp_N(n_e, n_t, p_r)
  sd <- 1
  
  a_est <- rbinom(1, n, p_a)/n
  f <- rbinom(1, 2*n, p_f)
  f_est <- f/(2*n)
  r_est <- 2*n/(rnbinom(1, 2*n, p_r) + 2*n)
  
  stat <- mu*a_est*sqrt(get_exp_N(n_e, n_t, r_est)*f_est)/sqrt((2*sd^2 + mu*mu*a_est*(1-a_est)))
  stat > c
}

#mean(replicate(100000, do_sim(2.4, phi=c(0.9976897, 0.8637934, 1.0000000))))
```

Now, for a fixed pilot sample size, see what error rates we can obtain by using different critical values.

```{r}
cs <- seq(2, 3, 0.05)

ocs30 <- NULL
des_a <- des_b <- rep(0.1,3)
for(c in cs){
  r <- get_ocs(c, n=30, x0=x0, x1=x1, mu=0.3, n_e=500, n_t=234, Ms = 10^seq(1:5), des_a=des_a, des_b=des_b)
  ocs30 <- rbind(ocs30, r[1:2])
  des_a <- r[3:5]; des_b <- r[6:8]
}

ocs30 <- sapply(cs, get_ocs, n=30, x0=x0, x1=x1, mu=0.3, n_e=500, n_t=234, Ms = 10^seq(1:5))
plot(ocs30)
#saveRDS(ocs30, "ex1_ocs_n30.Rda")

ocs50 <- sapply(cs, get_ocs, n=50, x0=x0, x1=x1, n_e=500, n_t=234)
#saveRDS(ocs50, "ex1_ocs_n50.Rda")
```

Try multi-bjective
```{r}
MO_f <- function(y, n, x0, x1, mu=0.3, n_e, n_t)
{
  cc <- y[1]; sd <- 1
  
  p_r <- y[2]; p_a <- y[3]; p_f <- y[4]; 
  alpha <- get_comp_ocs_cpp3(matrix(c(cc, p_r, p_a, p_f, sd), ncol=5), n, mu, n_e, n_t) #- 

  
  p_r <- y[5]; p_a <- y[6]; p_f <- y[7]; 
  beta <- 1 - get_comp_ocs_cpp3(matrix(c(cc, p_r, p_a, p_f, sd), ncol=5), n, mu, n_e, n_t) #- 

  
  return(-c(alpha, beta))
}

MO_g <- function(y, n, x0, x1, mu=0.3, n_e, n_t)
{
  cc <- y[1]; sd <- 1
  
  p_r <- y[2]; p_a <- y[3]; p_f <- y[4]; 
  exp_n <- get_exp_N(n_e, n_t, p_r)
  con_a <-  p_a*mu*sqrt(p_f*exp_n)/sqrt(2*sd^2 + mu^2 * p_a*(1-p_a))

  p_r <- y[5]; p_a <- y[6]; p_f <- y[7]; 
  exp_n <- get_exp_N(n_e, n_t, p_r)
  con_b <-  p_a*mu*sqrt(p_f*exp_n)/sqrt(2*sd^2 + mu^2 * p_a*(1-p_a))

  return(c(x0-con_a, con_b-x1))
}

opt <- nsga2(fn=MO_f, 7, 2, 
             n=30, x0=x0, x1=x1, mu=0.3, n_e=500, n_t=234,
             lower.bounds = c(2, rep(0.1,6)), upper.bounds = c(3, rep(1,6)),
             generations = 500,
             constraints = MO_g, cdim=2)

# Vecotrised - everything in C++
opt <- nsga2(fn=vec_f, 7, 2, 
             n=30, x0=x0, x1=x1, mu=0.3, n_e=500, n_t=234,
             lower.bounds = c(2, rep(0.1,6)), upper.bounds = c(3, rep(1,6)),
             generations = 10,
             #constraints = vec_g, cdim=2,
             vectorized = TRUE)

# Start the clock!
ptm <- proc.time()
z <- replicate(1000, MO_f(c(2.4,rep(0.9,6)), n=30, x0=x0, x1=x1, mu=0.3, n_e=500, n_t=234))
#z <- replicate(1, vec_f(matrix(rep(c(2.4,rep(0.9,6)),1000), ncol=7, byrow = T), n=30, x0=x0, x1=x1, mu=0.3, n_e=500, n_t=234))
proc.time() - ptm
# So, no benefits from vectorising

ggplot(data.frame(-opt$value), aes(X1,X2)) + geom_point() + geom_line() + coord_fixed() + ylim(c(0,1))
```

Now plot the results:
```{r}
ocs30 <- readRDS("ex1_ocs_n30.Rda")
ocs50 <- readRDS("ex1_ocs_n50.Rda")

df <- data.frame(a = c(ocs30[1,], ocs50[1,]), b=c(ocs30[2,], ocs50[2,]), n=c(rep(30, ncol(ocs30)), rep(50,ncol(ocs30))))
ggplot(df, aes(a, b, colour=as.factor(n))) + geom_point() + geom_line() +
  theme_minimal() + #xlim(c(0,0.5)) + ylim(c(0,0.5)) +
  scale_colour_manual(name=expression(n[p]), values=colours) +
  ylab(expression(beta)) + xlab(expression(alpha))

#ggsave("U:/Projects/MRC SDF/WP1/Papers/Feasibility test/Figures/ex_ocs.pdf", height=9, width=14, units="cm")
```
Take an example design and look at the sensitivity of error rates as the odds ratio increases.

```{r}
ors <- seq(1,10, 0.02)
sen_ocs <- sapply(ors, function(x) get_ocs_cor(2.5, n=30, x0, x1, mu=0.3, n_e=500, n_t=234, or=x))
df <- data.frame(or = ors, p=c(sen_ocs[1,], sen_ocs[2,]), oc=c(rep("a", ncol(sen_ocs)), rep("b", ncol(sen_ocs))))
saveRDS(df, "sen_ocs.Rda")
```

```{r}
df <- readRDS("sen_ocs.Rda")
df <- df[df$or <= 10,]

ggplot(df, aes(or, p, colour=oc)) + geom_point() +
  xlab("Odds ratio") + ylab("Error rate") +
  scale_colour_discrete(name=("Error type"), labels=c(expression(alpha), expression(beta))) +
  theme_minimal()

#ggsave("U:/Projects/MRC SDF/WP1/Papers/Feasibility test/Figures/ex_sens.pdf", height=9, width=14, units="cm")
```

We can plot the surface we are optimising over and see where the maximum error rates are obtained, say for $n=30$ and $c=2.5$:

```{r}
# Type I
df <- expand.grid(p_r=seq(0,1,0.01), p_a=seq(0,1,0.01))
df$p_f <- apply(df, 1, get_fup, xi=x0, mu=mu, n_e=235, n_t=235)
sub <- df[df$p_f <= 1 & df$p_r <=1,]
sub$a <- -apply(sub, 1, get_oc, n=30, cc=2.4, xi=x0, n_e=235, n_t=235)
max(sub$a)

#ptm <- proc.time()
#x <- replicate(500, get_oc(as.numeric(sub[10,1:2]), n=30, cc=2.4, xi=x0, n_e=800, n_t=235))
#proc.time() - ptm

ggplot(sub, aes(p_r, p_a, z=a)) + geom_contour(aes(colour=..level..)) +
  theme_minimal() + xlab("R") + ylab("A") + 
  guides(colour = guide_legend(title = expression()))

# Type II
df <- expand.grid(p_r=seq(0,1,0.03), p_a=seq(0,1,0.03))
df$p_f <- apply(df, 1, get_fup, xi=x1, mu=mu, n_e=800, n_t=235)
sub <- df[df$p_f <= 1 & df$p_r <= 1,]
sub$a <-  -apply(sub, 1, get_oc, n=30, cc=2.4, xi=x1, n_e=800, n_t=235)
min(sub$a)

ggplot(sub, aes(p_r, p_a, z=a)) + geom_contour(aes(colour=..level..)) +
  theme_minimal() + xlab("R") + ylab("Adherence") + 
  guides(colour = guide_legend(title = expression()))
```

We see that error rates appear to be maximised when $p_r = p_f = 1$ and $p_a$ is minimised. But this is not actually the case. Take the point (0.9968275,1,0.685), which gives $\alpha = 0.225$. In contrast, the point (0.9543184,1,0.7) gives a higher $\alpha = 0.261$. On the other hand, type II error does seem to be consistently maximised when $p_r = p_f = 1$.

### Example


```{r}
p <- seq(0,1,0.001)
n <- 30

get_lims <- function(p, n)
{
  x <- rbinom(100000, n, p)
  x <- x/n
  return(as.numeric(quantile(x, c(0.025, 0.1, 0.2, 0.5, 0.8, 0.9, 0.975))))
}

df <- as.data.frame(t(sapply(p, get_lims, n=n)))
names(df) <- c("q025", "q10", "q20", "q50", "q80", "q90", "q975")
df$p <- p

ggplot(df, aes(p, q50)) + 
  geom_ribbon(aes(ymin = q20, ymax = q80), alpha=0.2) +
  geom_ribbon(aes(ymin = q025, ymax = q975), alpha=0.2) +
  #geom_ribbon(aes(ymin = q20, ymax = q80), alpha=0.1) +
  geom_line() + 
  theme_minimal() +
  geom_vline(xintercept = 0.54, linetype=2) + geom_vline(xintercept = 0.75, linetype=2) +
  geom_hline(yintercept = 0.7)
```


### Evaluation

Under the new formulation, error rates are invariant with $n_e$, and depend on $\mu$ and $n_t$ only through the standardised effect. That is, if we change $\mu$ but then alter $n_t$ to give 90\% power again at the new $\mu$, the OCs are the same. So the scenarios we want to look at can fix $n_e$ at a suitably high value, fix $\mu=0.3$ (say), and then vary $n_t$. this variation can correspond to inflating the sample size to try and allow for some attrition. 

```{r}
get_plot_data <- function(null_power, n_t)
{
  print(null_power)
  p0 <- null_power; p1 <- 0.8

  x0 <- qnorm(p0) + qnorm(0.975)
  x1 <- qnorm(p1) + qnorm(0.975)
  
  cs <- seq(2, 3, 0.05)

  ocs30 <- sapply(cs, get_ocs, n=30, x0=x0, x1=x1, mu=mu, n_e=500, n_t=n_t)
  ocs50 <- sapply(cs, get_ocs, n=50, x0=x0, x1=x1, mu=mu, n_e=500, n_t=n_t)
  ocs100 <- sapply(cs, get_ocs, n=100, x0=x0, x1=x1, mu=mu, n_e=500, n_t=n_t)
  
  opt <- nsga2(get_ocs_pc, 3, 2, n=50, x0=x0, x1=x1,
             lower.bounds = c(0, 0, 0), upper.bounds = c(1, 1, 1),
              popsize = 100, generations = 100)
  
  df <- as.data.frame(rbind(t(ocs30), t(ocs50), t(ocs100), opt$value))
  names(df) <- c("a", "b")
  df$t <- c(rep("FT", 3*length(cs)), rep("PC", 100))
  df$np <- c(rep(30,length(cs)), rep(50,length(cs)), rep(100,length(cs)), rep(50,100))
  df$nul_pow <- null_power
  df$n_t <- n_t
  
  return(df)
}

df <- NULL
#for(i in c(234, 234*1.1, 234*1.2)){
for(i in c(234*1.1, 234*1.2)){
  for(j in c(0.6, 0.65, 0.7)){
    df <- rbind(df, get_plot_data(j, i))
  }
}

saveRDS(df, "eval_data.Rda")
```

Plot the results

```{r}
df <- readRDS("eval_data.Rda")

labels_nt <- c("234" = "234", "257.4" = "257", "280.8" = "281")

ggplot(df, aes(a, b, colour=as.factor(np), linetype=t)) + geom_line() + 
  facet_grid(n_t ~ nul_pow, labeller = labeller(n_t = labels_nt)) +
  theme_minimal() + coord_fixed() +
  xlab(expression(alpha)) + ylab(expression(beta)) +
  scale_color_manual(name=expression(n[p]), values=colours) +
  scale_linetype(name="Method", labels=c("Feasibility test", "Standard practice")) +
  theme(panel.spacing = unit(1, "lines"), legend.position="bottom") +
  scale_x_continuous(breaks=c(0, 0.25, 0.5, 0.75, 1)) 
  #geom_segment(aes(x = 0, y = 1, xend = 1, yend = 0), linetype=3, colour="black")

#ggsave("U:/Projects/MRC SDF/WP1/Papers/Feasibility test/Figures/eval.pdf", height=16, width=18, units="cm")
```
Pick out some specific numbers for comparison.
```{r}
df[df$nul_pow == 0.7 & df$a > 0.08 & df$a < 0.12 & df$t == "FT" & df$np == 30,]

df[df$n_t == 234 & df$a > 0.08 & df$a < 0.12 & df$t == "FT" & df$np == 30,]
```

Look at one example graph in more detail.
```{r}
sub <- df[df$nul_pow == 0.6 & df$n_t == "234",]

ggplot(sub, aes(a,b,colour=as.factor(np), linetype=t)) + geom_line() +
  theme_minimal() + coord_fixed() +
  xlab(expression(alpha)) + ylab(expression(beta)) +
  scale_color_manual(name=expression(n[p]), values=colours) +
  scale_linetype(name="Method", labels=c("Feasibility test", "Standard practice")) +
  scale_x_continuous(breaks=c(0, 0.2, 0.4, 0.6, 0.8, 1)) + 
  scale_y_continuous(breaks=c(0, 0.2, 0.4, 0.6, 0.8, 1)) 

#ggsave("U:/Projects/MRC SDF/WP1/Papers/Feasibility test/Figures/ex_ocs.pdf", height=9, width=14, units="cm")
```

Note that as we change $p_0$, the type I error will change for fixed $c$ but the type II error will remain fixed. So, looking at the case of $n_t = 235$, we can find the $c$ that gives us some specific power (say, 95\%) and then plot type I error as a function of null power.

```{r}
get_plot_data2 <- function(null_power, c, n)
{
  mu <- 0.3
  p0 <- null_power; p1 <- 0.8; 

  x0 <- qnorm(p0) + qnorm(0.975)
  x1 <- qnorm(p1) + qnorm(0.975)

  get_ocs(c, n=n, x0=x0, x1=x1, mu=mu, n_e=500, n_t=235)
}

pows <- seq(0.5, 0.8, 0.01)

# Manusally chosen critical values to give a type II error of 0.1 at p_1 = 0.8

df30 <- data.frame(t(sapply(pows, get_plot_data2, c=2.5, n=30)))
df30 <- cbind(df30, p=pows, np=rep(30, length(pows)))

df50 <- data.frame(t(sapply(pows, get_plot_data2, c=2.5725, n=50)))
df50 <- cbind(df50, p=pows, np=rep(50, length(pows)))

df100 <- data.frame(t(sapply(pows, get_plot_data2, c=2.642, n=100)))
df100 <- cbind(df100, p=pows, np=rep(100, length(pows)))

# What do these ciritcal values correspond to, in terms of main trial power or sample size required?
pnorm(c(2.5, 2.5725, 2.642) - qnorm(0.975))

df <- rbind(df30, df50, df100)
names(df)[1:2] <- c("a", "b")
saveRDS(df, "eval_data2.Rda")
```

```{r}
df <- readRDS("eval_data2.Rda")

ggplot(df, aes(p, a, colour=as.factor(np))) + geom_line() +
  ylab(expression(alpha)) + xlab(expression(p[0])) +
  scale_color_manual(name=expression(n[p]), values=colours) +
  theme_minimal()

#ggsave("U:/Projects/MRC SDF/WP1/Papers/Feasibility test/Figures/eval2.pdf", height=9, width=14, units="cm")
```

### Comparison

Making the comparison with the standard progression criteria decision rule:

```{r}
mu <- 0.3
p0 <- 0.7; p1 <- 0.8

x0 <- (qnorm(p0) + qnorm(0.975))
x1 <- (qnorm(p1) + qnorm(0.975))

get_oc_pc <- function(phi, cs, n, xi, tI, s, f, a)
{
  p_r <- phi[1]; p_a <- phi[2]; sd <- 1
  exp_n <- get_exp_N(n_e, n_t, p_r)
  p_f <- (2*sd^2 + mu^2 *p_a*(1-p_a))*xi^2/((p_a*mu)^2*exp_n)
  if(p_f <= 1){
    (tI*-1 + !tI*1)*pnbinom(s, 2*n, p_r)*(1- pbinom(f, 2*n, p_f))*(1 - pbinom(a, n, p_a))
  } else {
    10000
  }
}

get_ocs_pc <- function(cs, n, x0, x1)
{
  df <- as.data.frame(sobol(1000,2)); names(df) <- c("p_r", "p_a")
  df$p_f <- apply(df, 1, get_fup, xi=x0, mu=mu, n_e=n_e, n_t=n_t)
  sub_n <- df[df$p_f <= 1,]

  df <- as.data.frame(sobol(1000,2)); names(df) <- c("p_r", "p_a")
  df$p_f <- apply(df, 1, get_fup, xi=x1, mu=mu, n_e=n_e, n_t=n_t)
  sub_a <- df[df$p_f <= 1,]
  
  # First, get the numbers corresponding to the 
  # critical values and their respective probabilities
  
    # Recruitment
    s <- floor(2*n/cs[1] - 2*n)
    sub_n$a <- pnbinom(s, 2*n, sub_n[,1])
    # Adherence
    a <- cs[2]*n
    sub_n$a <- sub_n$a*(1 - pbinom(a, n, sub_n[,2]))
    # Follow-up
    f <- cs[3]*2*n
    sub_n$a <- sub_n$a*(1- pbinom(f, 2*n, sub_n[,3]))


    # Recruitment
    sub_a$a <- pnbinom(s, 2*n, sub_a[,1])
    # Adherence
    sub_a$a <- sub_a$a*(1 - pbinom(a, n, sub_a[,2]))
    # Follow-up
    sub_a$a <- sub_a$a*(1- pbinom(f, 2*n, sub_a[,3]))

    
    # Locally optimise using the worst error rate from the above as the start point
    
    opt_a <- optim(sub_n[which.max(sub_n$a),1:2], fn=get_oc_pc, lower = c(0.1,0.1), upper=c(1,1),
                 cs=cs, n=n, xi=x0, tI=T, s=s, f=f, a=a, method="L-BFGS-B")
    
    opt_b <- optim(sub_a[which.min(sub_a$a),1:2], fn=get_oc_pc, lower = c(0.1,0.1), upper=c(1,1),
                 cs=cs, n=n, xi=x1, tI=F, s=s, f=f, a=a, method="L-BFGS-B")
    
  
  c(-opt_a$value, 1-opt_b$value)
}

# For example,
get_ocs_pc(rep(0.8, 3), 30, x0, x1)
```
We can search over all possible progression criteria and plot the best available error rates.

```{r, eval=F}
opt <- nsga2(get_ocs_pc, 3, 2, n=50, x0=x0, x1=x1,
             lower.bounds = c(0, 0, 0), upper.bounds = c(1, 1, 1),
              popsize = 100, generations = 100)
plot(opt$value)

ocs <- data.frame(a = opt$value[,1], b = opt$value[,2])

#saveRDS(ocs, "ex1_ocs_n30_pc.Rda")
#saveRDS(ocs, "ex1_ocs_n50_pc.Rda")
```

Now plot the results:
```{r}
ocs <- t(readRDS("ex1_ocs_n50.Rda"))
ocs_pc <- readRDS("ex1_ocs_n50_pc.Rda")

df <- data.frame(a = c(ocs[,1], ocs_pc[,1]), b=c(ocs[,2], ocs_pc[,2]), 
                 t=c(rep("FT", nrow(ocs)), rep("PC",nrow(ocs_pc))))

ggplot(df, aes(a,b, colour=t)) + geom_point() +
  geom_abline(intercept = 1, slope = -1, linetype=2) + 
  xlab(expression(alpha)) + ylab(expression(beta)) +
  scale_color_discrete(name="", labels=c("Feasibility test", "Standard practice")) +
  coord_fixed() + theme_minimal()

#ggsave("U:/Projects/MRC SDF/WP1/Papers/Feasibility test/Figures/ex_comp.pdf", height=9, width=14, units="cm")
```

A differnt model for the main trial - assume that we will have $n^*$ eligible participants and a target sample size $n^t$, and that will cap the number we ever get. In our exressions for the variances of the group means, we need to revisit $E[N]$ where $N$ is the sample size of followed up participants. let $N_c$ be the number of consenters and $N_r$ the number actually recruited,  By the law of total expectation, 

$$
E[N_r] = E[N_r ~|~ N_c < n_t] Pr(N_c < n_t) + E[N_r ~|~ N_c \geq n_t]
$$
The second expectation is clearly $n_t$. For the first term, we have
$$
E[N_r ~|~ N_c < n_t] = \frac{\sum_{k=0}^{n_t-1} k{n_e \choose k} \phi_r^k (1-\phi_r)^{n_e - k} } {Pr(N_c < n_t)}
$$
Putting these together:

```{r}
# Expected value of n_r
get_exp_N <- function(n_e, n_t, phi_r)
{
  n_r <- 0:(n_t-1)
  sum(dbinom(n_r, n_e, phi_r)*n_r) + n_t*(1-pbinom(n_t-1, n_e, phi_r))
}
```

### Extensions

#### Correlated follow-up and adherence

First, we can relax the assumption of independance between the follow-up and adherence outcomes. We see that as the odds ratio increases, power decreases (this relationship is also noted in Bryant and Day). We might resonably restrict attention to postive odds ratios - we wouldn't expect adherence and follow-up to be inversly correlated. Then we should define type I error with an OR of 1, and type II with an OR of inf. This is very conservative - better to recommend a sensitivity analysis around some (possibly conservative) guess of the true OR?

```{r}
get_oc_cor <- function(phi, n, cc, xi, a=TRUE, mu=0.3, n_e, n_t, or)
{
  p_r <- phi[1]; p_a <- phi[2]; sd <- 1
  exp_n <- get_exp_N(n_e, n_t, p_r)
  p_f <- (2*sd^2 + mu^2 *p_a*(1-p_a))*xi^2/((p_a*mu)^2*exp_n)
  if(p_f <= 1){
    (a*-1 + !a*1)*get_comp_ocs_cpp4(cc, n, matrix(c(p_r, p_a, p_f, or), ncol=4), mu, n_e, n_t)
  } else {
    10000
  }
}

get_ocs_cor <- function(c, n, x0, x1, mu=0.3, n_e, n_t, or)
{
  df <- expand.grid(p_r=seq(0,1,0.02), p_a=seq(0,1,0.02))
  df$p_f <- apply(df, 1, get_fup, xi=x0, mu=mu, n_e=n_e, n_t=n_t)
  sub <- df[df$p_f <= 1 & df$p_r <=1,]
  sub$a <- -apply(sub, 1, get_oc_cor, n=n, cc=c, xi=x0, mu=mu, n_e=n_e, n_t=n_t, or=or)
  
  opt_a <- optim(sub[which.max(sub$a),1:2], fn=get_oc_cor, lower = c(0.1,0.1), upper=c(1,1),
                  cc=c, n=n, xi=x0, mu=mu, n_e=n_e, n_t=n_t, or=or, method="L-BFGS-B")
  
  # Type II
  df <- expand.grid(p_r=seq(0,1,0.02), p_a=seq(0,1,0.02))
  df$p_f <- apply(df, 1, get_fup, xi=x1, mu=mu, n_e=n_e, n_t=n_t)
  sub <- df[df$p_f <= 1 & df$p_r <= 1,]
  sub$a <-  -apply(sub, 1, get_oc_cor, n=n, cc=c, xi=x1, mu=mu, n_e=n_e, n_t=n_t, or=or)
  
  opt_b <- optim(sub[which.min(sub$a),1:2], fn=get_oc_cor, lower = c(0.1,0.1), upper=c(1,1),
                 cc=c, n=n, xi=x1, a=F, mu=mu, n_e=n_e, n_t=n_t, or=or, method="L-BFGS-B")
  
  c(-opt_a$value, 1-opt_b$value)
}

# For example,
get_ocs_cor(2.4, n=30, x0, x1, mu=0.3, n_e=500, n_t=234, or=1)
```

#### Unknown variance

```{r}
get_sd <- function(phi, xi, mu, n_e, n_t)
{
  p_r <- phi[1]; p_a <- phi[2]; p_f <- phi[3]
  exp_n <- get_exp_N(n_e, n_t, p_r)
  sqrt((p_f*(p_a*mu)^2*exp_n - mu^2 *p_a*(1-p_a)*xi^2)/(2*xi^2))
}

get_oc_var <- function(phi, n, cc, xi, a=TRUE, mu=0.3, n_e, n_t, M)
{
  p_r <- phi[1]; p_a <- phi[2]; p_f <- phi[3]; sd <- 1
  exp_n <- get_exp_N(n_e, n_t, p_r)
  #con <-  p_a*mu*sqrt(p_f*exp_n)/sqrt(2*sd^2 + mu^2 * p_a*(1-p_a))
  sd <- sqrt((p_f*(p_a*mu)^2*exp_n - mu^2 *p_a*(1-p_a)*xi^2)/(2*xi^2))
  if(!is.na(sd)){
    #(a*-1 + !a*1)*get_ocs_var_cpp(cc, n, matrix(c(p_r, p_a, p_f, sd), ncol=4), mu, n_e, n_t) + M*(xi-con)^2 
    (a*-1 + !a*1)*get_ocs_var_cpp(cc, n, matrix(c(p_r, p_a, p_f, sd), ncol=4), mu, n_e, n_t) + M*(max(0,0.8-sd)^2)
  } else {
    10000
  }
}

get_ocs_var <- function(c, n, x0, x1, mu, n_e, n_t, Ms, print.level=0)
{
  # Type I
  des <- c(1,1,1)
  for(m in Ms){
    opt_a <- optim(des, fn=get_oc_var, lower = c(0.1,0.1,0.1), upper=c(1,1,1),
                  cc=c, n=n, xi=x0, a=T, mu=mu, n_e=n_e, n_t=n_t, method="L-BFGS-B", M=m)
    if(print.level == 1) print(c(opt_a$par, get_sd(opt_a$par, x0, mu, n_e, n_t)))
    des <- opt_a$par
  }

  # Type II
  des <- c(1,1,1)
  for(m in Ms){
    opt_b <- optim(des, fn=get_oc_var, lower = c(0.1,0.1,0.1), upper=c(1,1,1),
                  cc=c, n=n, xi=x1, a=F, mu=mu, n_e=n_e, n_t=n_t, method="L-BFGS-B", M=m)
    if(print.level == 1) print(c(opt_b$par, get_sd(opt_b$par, x1, mu, n_e, n_t)))
    des <- opt_b$par
  }
  
  c(-opt_a$value, 1-opt_b$value)
}

get_ocs_var(c=2.4, n=30, x0=x0, x1=x1, mu=0.3, n_e=500, n_t=235, Ms = 10^seq(1:5), print.level = 1)
  
# Compare with simulation
sim <- function(phi)
{
  xi <- x1; mu <- 0.3; n <- 30; c <- 2.4; n_e <- 500; n_t <- 235
  p_r <- phi[1]; p_a <- phi[2]; p_f <- phi[3] 
  exp_n <- get_exp_N(n_e, n_t, p_r)
  sd <- sqrt((p_f*(p_a*mu)^2*exp_n - mu^2 *p_a*(1-p_a)*xi^2)/(2*xi^2))
  
  a_est <- rbinom(1, n, p_a)/n
  f <- rbinom(1, 2*n, p_f)
  f_est <- f/(2*n)
  r_est <- 2*n/(rnbinom(1, 2*n, p_r) + 2*n)
  var_est <- rchisq(1, f-1)*(sd^2)/(f-1)
  
  stat <- mu*a_est*sqrt(get_exp_N(n_e, n_t, r_est)*f_est)/sqrt((2*var_est + mu*mu*a_est*(1-a_est)))
  stat > c
}
  
phi <- c(0.8711703, 0.8642971, 0.9988547)
mean(replicate(100000, sim(phi)))
```

Find error rates for a range of c as before.
```{r}
cs <- seq(2, 3, 0.05)
ocs30 <- sapply(cs, get_ocs_var, n=50, x0=x0, x1=x1, mu=0.3, n_e=500, n_t=234, Ms = 10^seq(1:5))
plot(t(ocs30))

ocs30 <- as.data.frame(t(ocs30))
names(ocs30) <- c("a", "b")

#saveRDS(ocs30, "ex1_ocs_n30_var.Rda")
```

Now plot the results and compare with the case with no estimation of $\sigma$.
```{r}
ocs_var <- as.data.frame(readRDS("ex1_ocs_n30_var.Rda"))
names(ocs_var) <- c("a", "b")

df <- readRDS("eval_data.Rda")
sub <- df[df$nul_pow == 0.6 & df$n_t == "234" & df$np ==30, 1:2]

ocs_var$t <- "u_var"
sub$t <- "k_var"

df <- rbind(sub, ocs_var)

ggplot(df, aes(a, b, shape=t)) + geom_point() +
  xlab(expression(alpha)) + ylab(expression(beta)) +
  scale_shape_discrete(name="", labels=c("Known variance", "Unknown varaince")) +
  coord_fixed() + theme_minimal()

#ggsave("U:/Projects/MRC SDF/WP1/Papers/Feasibility test/Figures/unknown_var.pdf", height=9, width=14, units="cm")
```

We see empirically that error rates are maximised when $\sigma$ is minimised. So, firstly, we need to choose some lower bound for $\sigma$ that we want to control error rates for. Secondly, can we see analytucally why this is the case, and thus reduce the problem back to one of fixed $\sigma$ (at the chosen lower bound)? As $\sigma$ decreases, one of the three rate parameters will also have to decrease to stay on the hypothesis border. Broadly, error rates will be maximised when the variance of the statistic is maximised. As $\sgima$ decreases, the variance of the statistic will increase (via increase in the $1/\sigma$ term). Decresing a rate parameter (whilst being above 0.5) will increase the sample variance and therefore also the statistics variance. We can only show this all empirically at the moment:

```{r}
get_var_sd <- function(sd, n)
{
  var(1/sqrt(rchisq(10000,n-1)*(sd^2)/(n-1)))
}

sds <- seq(0.5,2,0.1)
vars <- sapply(sds, get_var_sd, n=20)
plot(sds, vars)
```
If we can show that, for the problems in our scope, this is always going to be the case then we have simplified the problem somewhat. But now we require everything to be conditional on a lower bound on the variance.


## References


## Extension - variances

We can incorporate estimatig variance components into our framework. 

```{r}
# Just one sd
power.t.test(delta=0.3, sd=1, power=0.9) # n_i = 234*2
# Previously, allowed for an inflated of 500 being null, and 350 being alt
power.t.test(delta=0.3, sd=1, power=0.5) # null
power.t.test(delta=0.3, sd=1, power=0.9) # alt

np <- 30
sds_n <- sqrt(1.45^2 * rchisq(10000, 2*np-1)/(2*np-1))
sds_a <- sqrt(1.15^2 * rchisq(10000, 2*np-1)/(2*np-1))

rules <- seq(0.8, 1.5, 0.01)
tIs <- sapply(rules, function(x) mean(sds_n < x))
tIIs <- sapply(rules, function(x) mean(sds_a > x))
plot(tIs, tIIs)


obj_f_null <- function(x, n_i)
{
  p_r <- x[1]; p_f <- x[2]; p_a <- x[3]; sd <- x[4]
  
  y <- p_a*0.3*sqrt(p_r*p_f)/sqrt(2*sd^2 + 0.3^2 * p_a*(1-p_a))
  
  pen <- 100000*(y - x0)^2 # x0 calculated below
  return( -c(x[1:3], -x[4]) + pen)
}

obj_f_alt <- function(x, n_i)
{
  p_r <- x[1]; p_f <- x[2]; p_a <- x[3]; sd <- x[4]
  
  y <- p_a*0.3*sqrt(p_r*p_f)/sqrt(2*sd^2 + 0.3^2 * p_a*(1-p_a))
  
  pen <- 100000*(y - x1)^2 # x0 calculated below
  return( -c(x[1:3], -x[4]) + pen)
}

# Generate a set approximating the null space boundary
opt <- nsga2(obj_f_null, 4, 4, n_i=n_i,
             lower.bounds = c(0, 0, 0, 0.5), upper.bounds = c(1, 1, 1, 2),
             popsize = 1000)
null <- as.data.frame(opt$par)
names(null) <- c("p_r", "p_f", "p_a", "sd")
ggplot(null, aes(p_r, p_f, colour=p_a)) + geom_point()

# Generate a set approximating the alternative space boundary
opt <- nsga2(obj_f_alt, 4, 4, n_i=n_i,
             lower.bounds = c(0, 0, 0, 0.5), upper.bounds = c(1, 1, 1, 2),
             popsize = 1000)
alt <- as.data.frame(opt$par)
names(alt) <- c("p_r", "p_f", "p_a", "sd")
ggplot(alt, aes(p_r, p_f, colour=p_a)) + geom_point()
```

```{r}
get_comp_ocs_var_MC <- function(rule, n, par, sigma, N)
{
  # par = (phi_r, phi_f, phi_a, mu, sd)
  
  # Simulate outcomes in the intervention arm
  # (af, a!f, !af, !a!f)
  m <- t(rmultinom(N, n, c(par[3]*par[2], par[3]*(1-par[2]), (1-par[3])*par[2], (1-par[3])*(1-par[2]))))
  
  # Simulate follow-up in the control arm
  f0 <- rbinom(N, n, par[2])
  
  # Simulate number of failures before recruitment was met
  s <- rnbinom(N, 2*n, par[1])
  
  # Simulate sample sds
  sds <- sqrt(par[4]^2 * rchisq(N, 2*n-1)/(2*n-1))
  
  df <- cbind(m,f0,s)
  
  ns <- rnorm(N)
  
  ts <- MC_cpp(df, ns, sigma, par[4], n)[,3]
  
  ts2 <- 234*ts/(sds^2)
  ts2[is.na(ts2)] <- -10
  
  alpha <- mean(ts2 > rule)
  return(alpha)
}

get_comp_ocs_var_MC(160, n=30, par=as.numeric(null[200,]), sigma=1, N=1000)
get_comp_ocs_var_MC(160, n=30, par=as.numeric(alt[200,]), sigma=1, N=1000)

get_max_comp_ocs_eff <- function(rule, n, sigma, null, alt)
{
  N <- 1000
  # get max alpha
  a <- max(apply(null, 1, function(x, rule, n, sigma) get_comp_ocs_var_MC(rule, n, x, sigma, N), rule=rule, n=n, sigma=sigma))

  # get max beta
  b <- 1 - min(apply(alt, 1, function(x, rule, n, sigma) get_comp_ocs_var_MC(rule, n, x, sigma, N), rule=rule, n=n, sigma=sigma))
  
  return(c(a,b))
}

# For example
get_max_comp_ocs_eff(rule=140, n=50, sigma=1, null, alt)
```

Note that there is a natural _upper_ limit for the variance when we compute our hypotheses - as the variance increases (gets worse), the other three parameters have to increase (get better) to compensate, but they can't exceed 1. So we can stick any upper limit into our optimiser. But the lower limit is more sensitive - as variance decreases, we allow much lower rates into our hypotheses, and this leads to increases in the error rates. So we need to choose a variance, below which we are not interested in controlling errors. 

## Extension - using the pilot estimate

So far, we have assumed that the size of the main trial has been fixed in advance, and defined our hypotheses based on the power that the fixed size will obtain. After the pilot we will have an estimate - also our test sttaistic - of the inflated sample size the main trial will need to obtain 80\% power, so it might be reasonable to use this for our main trial. In particular, we might restrict ourselves to inflations of the original suggestion. Togther with the test ruling out results below the critical value, we are then controlling the probability of being in segments of the power curves corresponding to our null and alternatives. 

```{r}
get_pow <- function(n, pr)
{
  pnorm(0.3*pr[3]*sqrt(n*pr[1]*pr[2])/sqrt(2*pr[4]^2 + 0.3^2 * pr[3]*(1-pr[3])) - qnorm(0.975))
}

pr <- as.numeric(null[100,])

# Check against simulation
f <- function(n_i, pr)
{
  ns <- rbinom(2, round(n_i), pr[1]*pr[2]); 
  a <- rbinom(1, ns[2], pr[3])
  
  y_1 <- c(rnorm(a, 0.3, pr[4]), rnorm(ns[2]-a, 0, pr[4]))
  y_0 <- rnorm(ns[1], 0, pr[4])
  return(t.test(y_1, y_0, alternative = "greater", conf.level = 0.975)$p.value < 0.025)
}

get_pow(n_i, pr)
mean(replicate(10000, f(n_i, pr)))
```
How variable are the power functions in each of our hypotheses sample sets? Pick a few at random and plot:
```{r}
ns <- seq(10,700,20)
df <- NULL
for(i in 1:200){
  pr <- as.numeric(null[i,])
  pows <- sapply(ns, function(x) get_pow(x, pr))
  df <- rbind(df, cbind(ns, pows, r=rep(i, length(ns))))
}

ggplot(as.data.frame(df), aes(ns, pows, group=r)) + geom_line(alpha=0.1) +
  geom_hline(yintercept = 0.5, linetype=2) + geom_vline(xintercept = 174, linetype=2) + 
  theme_minimal()
```

This variability is coming from the optimisation not always being exact. Recall that we defined the null as those points in the parameter space such that we get a power of 0.5 with $n^*$. But if we have two points $\phi_0, \phi_1$ which both do this, they will give equal power for all other $n$. This is clear if we re-write the power for the main trial as
$$
\Phi \left( \sqrt{n^*} x - z_{1-\alpha} \right),
$$
where
$$
x = \frac{ \phi_a\mu \sqrt{\phi_r \phi_f} } {\sqrt{2\sigma^2 + \mu^2 \phi_a(1-\phi_a)}}.
$$
So, no matter how it is composed, it is the facotr $x$ which determines the power curve. So we can plot our exact hypotheses in terms of the power curves. For $n^* = 174$, 
```{r}
ns <- seq(10,700,2)

x0 <- (qnorm(0.6) + qnorm(0.975))/sqrt(235)
x1 <- (qnorm(0.8) + qnorm(0.975))/sqrt(235)

p0s <- sapply(ns, function(n) pnorm(sqrt(n)*x0 - qnorm(0.975)))
p1s <- sapply(ns, function(n) pnorm(sqrt(n)*x1 - qnorm(0.975)))

df <- data.frame(n=rep(ns,2), p=c(p0s, p1s), h=c(rep("null", length(ns)), rep("alt", length(ns))))

ggplot(df, aes(n, p, colour=h)) + geom_line() + geom_vline(xintercept = 174, linetype=2) +
  theme_minimal()
```
Now, if we allow the sample size of the main trial to be chosen as a result of the pilot, we can think of our test not as excluding trials which will have 50\% power or less while maintaining those with 80\% power or more; rather, we are excluding trials which will get us somehwere on the null power curve or worse, etc. If we say that the main sample should not be less than $n^*$ even if the pilot sample says that's what is needed, then we can narrow it down to some sections on these curves.

```{r}
lo <- 174; hi <- 300 # hi is the critical value used in our test
sub <- df[df$n >= lo & df$n <= hi, ]

ggplot(df, aes(n, p, colour=h)) + geom_line() + geom_vline(xintercept = 174, linetype=2) +
  geom_line(data=sub, size = 2) +
  theme_minimal()
```

## Extension - assessing efficacy

If we would like to simultaneously assess the effectiveness of the intervention alongside the feasibility of the trial, we must reconsider how we define our hypotheses. One way to approach this is to define a value function over the two dimensional space of effectiveness and power of the main trial (assuming the sample size is fixed). If we can say that value in one dimension should be linear when the other attribute is fixed at some level, we get a value function
$$
v(\beta, \phi_a \mu) = \frac{(1-\beta) \phi_a \mu}{\mu^*},
$$
where $\mu^*$ is some (arbitrary) maximum value. This will give a value of 1 at the point $(0, \mu^*)$, and 0 whenever $\beta=1$ or $\mu=0$.

```{r}
# A value function for power and effectiveness
get_value <- function(x)
{
  pow <- x[1]; eff <- x[2]
  eff_max <- 1
  # End value for maximal eff is just pow
  # value is linear going from pow = 0 to pow = 1
  v <- ((eff+0.3)/eff_max)*pow
  
  # true MCID is about 0.18 - 0 value for all powers here
  # grad of v as a function of mu determined by fixed power
  m <- pow*2-1
  c <- -m*0.18
  v <- (c+m*eff)*eff
  return(v)
}

df <- expand.grid(pow=seq(0,1,0.05), eff=seq(0,1,0.05))
df$v <- apply(df, 1, get_value)

v_n <- get_value(c(0.3, 0.3)) # null value
v_a <- get_value(c(0.9, 0.3)) # alternative value
df$h <- ifelse(df$v < v_n, "N", ifelse(df$v > v_a, "A", "-"))

ggplot() + geom_contour(data=df, aes(eff, pow, z=v)) + coord_fixed() +
  geom_point(data=df, aes(eff, pow, colour=h), alpha=0.2)
  

  #geom_point(data=data.frame(eff=c(0.4264519, 0.4826196), pow=c(0.415072, 0.4984145)), aes(eff,pow))
```

[Note - we are treating power as a deterministic quantity here, hence using a value function (which encodes preferences under certainty) as opposed to a utility function (which encodes preferences under uncertainty). But if we extend the model slightly, we can view power as a simple gamble between a 0 treatment improvement (when we fail to reject the null and stick with standard care) and a $\mu$ treatment improvement. If we were to use the same function to describe our preferences, this would imply we are risk-nuetral with regards to the level of treatment imporvement we will obtain after the trial. This may be rather unlikely, particularly from the perspective of an individual who stands to benefit from such an improvement.]

With this value function and these thresholds for the null and alternative hypotheses, we can now approximate these again:
```{r}
obj_f_null2 <- function(x, n_i)
{
  p_r <- x[1]; p_f <- x[2]; p_a <- x[3]; mu <- x[4]
  eff_n <- n_i*(p_r*p_f*p_a^2)
  eff <- mu*p_a
  pow <- 0
  if(eff_n>1) pow <- power.t.test(n=eff_n, delta=mu, sd=1)$power
  v <- get_value(c(pow, eff))
  pen <- 100000*(v > 0.3)
  -x + pen
}

obj_f_alt2 <- function(x, n_i)
{
  p_r <- x[1]; p_f <- x[2]; p_a <- x[3]; mu <- x[4]
  eff_n <- n_i*(p_r*p_f*p_a^2)
  eff <- mu*p_a
  pow <- 0
  if(eff_n>1) pow <- power.t.test(n=eff_n, delta=eff, sd=1)$power
  v <- get_value(c(pow, eff))
  pen <- 100000*(v < 0.48)
  x + pen
}

opt <- nsga2(obj_f_null2, 4, 4, n_i=n_i,
             lower.bounds = c(0, 0, 0, 0), upper.bounds = c(1, 1, 1, 0.5),
             popsize = 1000)
null <- as.data.frame(-opt$value)
names(null) <- c("p_r", "p_f", "p_a", "mu")
plot(null)

opt <- nsga2(obj_f_alt2, 4, 4, n_i=n_i,
             lower.bounds = c(0, 0, 0, 0), upper.bounds = c(1, 1, 1, 0.5),
             popsize = 1000)
alt <- as.data.frame(opt$value)
names(alt) <- c("p_r", "p_f", "p_a", "mu")
plot(alt)
```

Given these hypotheses, we can extend our test statistic to
$$
T = S \times \hat{\mu}.
$$
The power function can be derived by a simple extension of the above formula. There, we had a term $Pr\bigg[ \hat{\phi_r}\frac{f}{2n_p}\frac{a^2}{n^2} > c \mid f, a, \phi \bigg]$, wheras now we must evaluate 
$$
Pr\bigg[ \hat{\mu}\hat{\phi}_a\hat{\phi_r}\frac{f}{2n_p}\frac{a^2}{n^2} > c \mid f, a, \phi, \mu \bigg] =
Pr\bigg[ \frac{\hat{\mu} f a^3}{(r_- + 2n_p)n_p^3} > c \mid f, a, \phi, \mu \bigg].
$$
The condition reduces to
$$
r_- < \frac{\hat{\mu}fa^3 - 2n_p^4c}{cn_p^3}
$$
Thus, 
$$
\begin{aligned}
Pr \left[ r_- < \frac{\hat{\mu}fa^3 - 2n_p^4c}{cn_p^3} \mid f, a, \phi, \mu \right] &= 
\int Pr \left[ r_- < \frac{\hat{\mu}fa^3 - 2n_p^4c}{cn_p^3} \mid \hat{\mu}, f, a, \phi, \mu \right] f(\hat{\mu} \mid \phi, \mu) d\hat{\mu} \\
&= \int F_{r_-}\left(\frac{\hat{\mu}fa^3 - 2n_p^4c}{cn_p^3} \mid \hat{\mu}, f, a, \phi, \mu \right) f(\hat{\mu} \mid \phi, \mu) d\hat{\mu}.
\end{aligned}
$$
Assymptotically, the approximate sampling distribution for the mean effect ITT complete case estimate depends on the numbers of adherers who are successfully followed up. That is, we need more than just the number of adherers and the number of successful follow-ups. If we denote by $f_0, f_1$ the numbers followed up in each arm, and by $a_{1,f}$ the number in the intervention arm who both adhere and are followed-up, then

$$
f(\hat{\mu} \mid a_{1,f}, f_0, f_1, \phi, \mu) \approx N\left(\frac{a_{1,f}}{f_1}\mu, \frac{\sigma^2}{f_0 + f_1} \right).
$$
This presents a computational challange compared with the previous case where we didn't look at efficacy, becasue now we must enumerate over all possible ways to allocate the participants into the two-by-two table of adhering and following up. For moderate sample sizes this will lead to a huge number of cases, each of which requiring the above integral to be solved. So, to keep things tractable, we will use an MC approximation.

The code below simulates $N$ trial outcomes. We first simulate the adherence and follow-up in the intervention group using a multinomial distribution, where we have assumed that these outcomes are independant (i.e. knowing a participant has adhered tells us nothing about whether or not the will be followed up). Then we simulate follow-up in the control arm, and the number of failed recruitment attampts needed to reach the target sample size of $2n$. Finally we simulate the observed mean difference, then calculate the statistic $T$ and compare with the threshold value. Note that to calculate the observed differences we first simulate $N$ standard normal deviates, and then translate these into the required distribution, recalling that

$$
X \sim N(0,1) \rightarrow \sqrt{y}X + z \sim N\left(z , y^2\right).
$$
Once the data are simulated we do the tranformations and computations in c++ to keep things relatively fast.

```{r}
get_comp_ocs_eff_MC <- function(rule, n, par, sigma, N)
{
  # par = (phi_r, phi_f, phi_a, mu)
  
  # Simulate outcomes in the intervention arm
  # (af, a!f, !af, !a!f)
  m <- t(rmultinom(N, n, c(par[3]*par[2], par[3]*(1-par[2]), (1-par[3])*par[2], (1-par[3])*(1-par[2]))))
  
  # Simulate follow-up in the control arm
  f0 <- rbinom(N, n, par[2])
  
  # Simulate number of failures before recruitment was met
  s <- rnbinom(N, 2*n, par[1])
  
  df <- cbind(m,f0,s)
  
  ns <- rnorm(N)
  
  ts <- MC_cpp(df, ns, sigma, par[4], n)
  ts <- cbind(ts, (ts[,1]+0.3)*ts[,2])
  #hist(ts[,3])
  ts[is.na(ts[,5]),5] <- 0
  
  alpha <- mean(ts[,5] > rule)
  return(alpha)
}

#df <- as.data.frame(ts[1:1000,])
#df$V5 <- df$V1*df$V2

#ggplot(df, aes(V1, V2, colour=V4)) + geom_point() + coord_fixed()
```

Now getting the maxmimum and minimum error rates:
```{r}
get_max_comp_ocs_eff <- function(rule, n, sigma, null, alt)
{
  N <- 1000
  # get max alpha
  a <- max(apply(null, 1, function(x, rule, n, sigma) get_comp_ocs_eff_MC(rule, n, x, sigma, N), rule=rule, n=n, sigma=sigma))

  # get max beta
  b <- 1 - min(apply(alt, 1, function(x, rule, n, sigma) get_comp_ocs_eff_MC(rule, n, x, sigma, N), rule=rule, n=n, sigma=sigma))
  
  return(c(a,b))
}

# For example
get_max_comp_ocs_eff(rule=0.5, n=100, sigma=1, null[1:100,], alt[1:100,])
```

What kind or error rates are possible as we alter the progression criteria?

```{r, eval=F}
cs <- seq(0.1, 0.6, 0.05)
df <- as.data.frame(cbind(cs, t(sapply(cs, get_max_comp_ocs_eff, n=100, sigma=1, null=null, alt=alt))))
names(df)[2:3] <- c("a", "b")

ggplot(df, aes(a,b)) + geom_point() + coord_fixed() + geom_abline(intercept = 1, slope = -1)
```
