---
title: "A hypothesis test of feasibility for pilot trials assessing recruitment, follow-up and adherence rates"
author: "D. T. Wilson"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(ggplot2)
require(pso)
require(mco)
require(RColorBrewer)
require(rgenoud)
require(randtoolbox)
require(gganimate)
require(Rcpp)
cols <- brewer.pal(8, "Dark2")
```

## Introduction

This RMarkdown document contains the R code which generates the results and figures included in the manuscript of the same name, plus supplementary material.

## Example

### Hypotheses

We first find the threshold values $x_0, x_1$ which correspond to obtaining 60 and 80% power and set our mean difference, $\mu$.

```{r}
mu <- 0.3
p0 <- 0.65; p1 <- 0.8

x0 <- (qnorm(p0) + qnorm(0.975))
x1 <- (qnorm(p1) + qnorm(0.975))
```

These in turn define our hypotheses, $\hat{\Phi}_0$ and $\hat{\Phi}_1$. We can visualise the boudaries of these spaces:

```{r}
get_exp_N <- function(n_e, n_t, phi_r)
{
  # Calculate the expected number recruited into the main trial
  n_r <- 0:(n_t-1)
  sum(dbinom(n_r, n_e, phi_r)*n_r) + n_t*(1-pbinom(n_t-1, n_e, phi_r))
}

get_fup <- function(phi, xi, mu, n_e, n_t)
{
  # For some recruitment and adherence rate, find the follow-up rate which will lie on
  # the boundary of the hypothesis defined by xi
  p_r <- phi[1]; p_a <- phi[2]; sd <- 1
  exp_n <- get_exp_N(n_e, n_t, p_r)
  (4*sd^2 + 2*mu^2 *p_a*(1-p_a))*xi^2/((p_a*mu)^2*exp_n)

}

# Null
df <- expand.grid(p_r=seq(0,1,0.01), p_a=seq(0,1,0.01))
df$p_f <- apply(df, 1, get_fup, xi=x0, mu=mu, n_e = 1000, n_t =514)
sub <- df

# Alternative
df <- expand.grid(p_r=seq(0,1,0.01), p_a=seq(0,1,0.01))
df$p_f <- apply(df, 1, get_fup, xi=x1, mu=mu, n_e = 1000, n_t =514)
sub_a <- df

# Combine for plotting
sub$h <- "Null"
sub_a$h <- "Alternative"
sub <- rbind(sub, sub_a)
# Discard any infeasible points
sub <- sub[sub$p_f <= 1,]

p <- ggplot(sub, aes(p_r, p_a, z=p_f, colour=p_f)) +
  geom_contour(aes(colour=..level..), breaks=c(0.5,0.6,0.7,0.8,0.9,1)) +
  theme_minimal() + xlab("Recruitment") + ylab("Adherence") + 
  guides(colour = guide_legend(title = "Follow-up")) +
  facet_grid(. ~ factor(h, levels=c("Null", "Alternative"))) +
  theme(panel.spacing = unit(1, "lines"))
p
#direct.label(p, "last.qp")

ggsave("./paper/figures/hyps.pdf", height=9, width=14, units="cm")
ggsave("./paper/figures/hyps.eps", height=9, width=14, units="cm", device = cairo_ps())
```

### Local error rates

Having defined our hypotheses, we can now calculate local type I and II error rates for a proposed pilot trial with sample size $n$, critical value $c$, and at two specific points in the null and alternative hypotheses.

```{r}
# Using a c++ implemntation of forumla (X)
Rcpp::sourceCpp('./src/comp_ocs.cpp')

get_ocs <- function(y, n, x0, x1, mu=0.3, n_e, n_t)
{
  # Input y = (c, [point in null space], point in alternative space])
  c <- y[1]; sd <- 1
  
  # Probability of +ve result at the null point
  p_r <- y[2]; p_a <- y[3]; p_f <- y[4]; 
  alpha <- get_comp_ocs_cpp3(matrix(c(c, p_r, p_a, p_f, sd), ncol=5), n, mu, n_e, n_t)

  # Probability of -ve result at the alternative point
  p_r <- y[5]; p_a <- y[6]; p_f <- y[7]; 
  beta <- 1 - get_comp_ocs_cpp3(matrix(c(c, p_r, p_a, p_f, sd), ncol=5), n, mu, n_e, n_t)

  # Returning negative values for optimisation later
  return(-c(alpha, beta))
}

# For example,
get_ocs(c(2.4, 0.64, 0.73, 0.8769883, 0.96, 1, 0.7422108), 30, x0, x1, 0.3, 1000, 514) 
```

### Global error rates

To solve the multi-objective optimisation problem described in the paper, we need to implment a constriant function to limit the search to our null and alternative hypotheses. The function returns a value for the null (alternative) point, being negative if the point is not within the null (alternative) hypothesis.

```{r}
get_constr <- function(y, n, x0, x1, mu=0.3, n_e, n_t)
{
  sd <- 1
  
  p_r <- y[2]; p_a <- y[3]; p_f <- y[4]; 
  exp_n <- get_exp_N(n_e, n_t, p_r)
  con_a <-  p_a*mu*sqrt(p_f*exp_n)/sqrt(4*sd^2 + 2*mu^2 * p_a*(1-p_a))

  p_r <- y[5]; p_a <- y[6]; p_f <- y[7]; 
  exp_n <- get_exp_N(n_e, n_t, p_r)
  con_b <-  p_a*mu*sqrt(p_f*exp_n)/sqrt(4*sd^2 + 2*mu^2 * p_a*(1-p_a))

  return(c(x0-con_a, con_b-x1))
}

# For example, change the alternative point from above so it is outside the alternative hypothesis
get_constr(c(2.4, 0.64, 0.73, 0.8769883, 0.96, 0.9, 0.7422108), 30, x0, x1, 0.3, 1000, 514)
```

We can now sole the problem using the NSGA-II algrothim from the "mco" package. Note that while there is an option to use a fully vectorised version (i.e. with the objective and constrained functions implemented in c++), this did not lead to any efficiency gain and so we have used R functions for greater transparancy.

```{r, eval = F}

ptm <- proc.time()

opt <- nsga2(fn=get_ocs, 7, 2, 
             n=50, x0=qnorm(0.65) + qnorm(0.975), x1=x1, mu=0.3, n_e=1000, n_t=514,
             lower.bounds = c(2, rep(0.1,6)), upper.bounds = c(3, rep(1,6)),
             generations = 1:200, popsize = 100,
             constraints = get_constr, cdim=2)

proc.time() - ptm

#   user  system elapsed 
# 595.55    0.75  602.27 

#saveRDS(opt, "./data/opt.Rda")
```

The algorithm returns a set of `popsize` solutions, each containing a critical value, a point in the null, and a point in the alternative. Each solution has a type I and II error, which we can plot to visualise the error rates available as we vary the critical value but keep the pilot sample size fixed.

```{r}
opt <- readRDS("./data/opt.Rda")

df <- data.frame(a=-opt[[200]]$value[,1], b=-opt[[200]]$value[,2])
df2 <- data.frame(a=-opt[[180]]$value[,1], b=-opt[[180]]$value[,2])

ggplot(df, aes(a, b)) + geom_point() + geom_line() +
  geom_line(data=df2, colour="red") +
  xlab(expression(alpha)) + ylab(expression(beta)) +
  theme_minimal() + 
  scale_y_continuous(breaks = seq(0,1,0.2)) + scale_x_continuous(breaks = seq(0,1,0.2)) +
  coord_fixed() #+ 
  # point evaluated with c=2.6 and OR=1
  #geom_point(data=data.frame(a=0.1285390, b=0.1496154), colour="green") +
  # point evaluated with c=2.6 and OR=5
  #geom_point(data=data.frame(a=0.1285390, b=0.3452193), colour="blue")
```

Try a nested optimisation:

```{r, eval=T}
get_oc2 <- function(phi, n, c, xi, a=TRUE, mu=0.3, n_e, n_t)
{
  p_r <- phi[1]; p_a <- phi[2]; sd <- 1
  if(any(c(p_r,p_a) < 0) | any(c(p_r,p_a) > 1) ){return(1000)}
  exp_n <- get_exp_N(n_e, n_t, p_r)
  p_f <- (4*sd^2 + 2*mu^2 *p_a*(1-p_a))*xi^2/((p_a*mu)^2*exp_n)
  if(any(c(p_r,p_a,p_f) < 0) | any(c(p_r,p_a,p_f) > 1) ){
    1000
  } else {
    (a*-1 + !a*1)*get_comp_ocs_cpp3(matrix(c(c, p_r, p_a, p_f, sd), ncol=5), n, mu, n_e, n_t)
  }
}

get_ocs2 <- function(c, n, x0, x1, mu=0.3, n_e, n_t)
{
  opt_a <- psoptim(c(0.9,0.9), fn=get_oc2,
                 c=c, n=n, xi=x0, a=T, mu=mu, n_e=n_e, n_t=n_t)
  
  opt_b <- psoptim(c(0.9,0.9), fn=get_oc2,
                 c=c, n=n, xi=x1, a=F, mu=mu, n_e=n_e, n_t=n_t)

  c(-opt_a$value, 1-opt_b$value)
}

get_ocs2(2.5, n=30, x0, x1, mu=0.3, n_e=1000, n_t=514)
  
```

The function does not by default report any information which could be used to assess convergence. We can do so by extracting the solution set obtained at each iteration in the final portion of the search (say, the last quarter) and for each set measuring its corresponding dominated hypervolume. If the algorithm has converged, this measure should platue towards the end of the search.

```{r}
DHs <- NULL
for(i in 1:length(opt)){
  dh <- dominatedHypervolume(-opt[[i]]$value, c(1,1))
  DHs <- c(DHs, dh)
}
plot(DHs)
```

It may be that error rates are being maximised at the same points in the null and alternative hypotheses for different values of $c$. When we plot the points, we see that this is not the case; error rates are maximised at a variety of locations in the hypotheses.

```{r}
ps <- data.frame(rbind(opt[[200]]$par[,2:4], opt[[200]]$par[,5:7]))
names(ps) <- c("p_r", "p_a", "p_f")
ps$h <- c(rep("Type I", 100), rep("Type II", 100))

ggplot(ps, aes(p_r, p_a, colour=p_f)) + geom_point() +
  facet_grid(. ~ h) +
  labs(x=expression(p[r]), y=expression(p[a]), colour=expression(p[f])) +
  theme_minimal()
```

If we choose a $c$ based on the above OC curve, how will error rates change as we vary $n_e$ and $n_t$?

```{r}
# 2.6325915 gives OCs of 0.11, 0.20
get_ocs2(c=2.6, n=50, x0, x1, mu=0.3, n_e=1000, n_t=514)
get_ocs2(c=2.6, n=50, x0, x1, mu=0.3, n_e=1000, n_t=714)
get_ocs2(c=2.6, n=50, x0, x1, mu=0.3, n_e=700, n_t=514)

```


### Alternative perspective

Rather than keeping the hypotheses fixed and looking for the error rates which can be obtained, we could instead constrain the type II error at some desired value and then look at how much type I error we will get for different hypotheses. Specifically, we can find the $c$ which will give a desired type II in the pilot for a fixed alternative defined by $p_1$, and then find the type I which results from a range of $p_0$.

```{r, eval=F}
# Manually find a c that gives approx 90% pilot power
psoptim(rep(NA,2), fn=get_oc2,
                 c=2.46, n=30, xi=x1, a=F, mu=0.3, n_e=1000, n_t=514,
        control=list(trace=1, reltol = 0.01, maxit = 200))
cs <- c(2.46, 2.53, 2.61)

# For a range of p_0 and n_p, find the type I error
df <- NULL
prev <- c(0.9, 0.9)
for(n_p in c(30, 50, 100)){
  for(p0 in seq(0.5, 0.8, 0.01)){
    c <- cs[which(c(30, 50, 100) == n_p)]
    x0 <- qnorm(p0) + qnorm(0.975)
    opt <- psoptim(rep(NA,2), fn=get_oc2,
                 c=c, n=n_p, xi=x0, a=T, mu=0.3, n_e=1000, n_t=514,
                 control=list(trace=0, reltol = 0.01, maxit = 200))
    tI <- -opt$value
    print(tI)
    df <- rbind(df, c(n_p, p0, tI))
  }
}

df <- data.frame(n_p = df[,1], p0 = df[,2], a = df[,3])

saveRDS(df, "./data/alt_data.Rda")
```

```{r}
df <- readRDS("./data/alt_data.Rda")

ggplot(df, aes(p0, a, colour=as.factor(n_p))) + geom_line() +
  scale_color_manual(name=expression(n[p]), values=cols) +
  xlab(expression(p[0])) + ylab(expression(alpha)) +
  theme_minimal()

ggsave("./paper/figures/alt.pdf", height=9, width=14, units="cm")
ggsave("./paper/figures/alt.eps", height=9, width=14, units="cm", device = cairo_ps())
```

### Comparator

We can comapre the error rate curve of the proposed method against that of the current approach, where the three parameter estimates are compared against seperate progression criteria thresholds and a "go" decision is made only if they are all exceeded. The optimisation strategy is based on a simple grid search at both the outer (over the progression criteria) and inner (over the hypotheses) levels. 

```{r}
get_pf <- function(phi, xi, mu, n_e, n_t)
{
  p_r <- phi[1]; p_a <- phi[2]; sd <- 1
  if(any(c(p_r,p_a) < 0) | any(c(p_r,p_a) > 1) ){return(1000)}
  exp_n <- get_exp_N(n_e, n_t, p_r)
  (4*sd^2 + 2*mu^2 *p_a*(1-p_a))*xi^2/((p_a*mu)^2*exp_n)
}

get_oc_pc <- function(cs, n, m, tI)
{
  # Recruitment
  s <- floor(2*n/cs[1] - 2*n)
  # Adherence
  a <- cs[2]*n
  # Follow-up
  f <- cs[3]*2*n
  
  (tI*-1 + !tI*1)*pnbinom(s, 2*n, m[,1])*(1- pbinom(f, 2*n, m[,3]))*(1 - pbinom(a, n, m[,2]))
}

# for example,

m0 <- expand.grid(p_r = seq(0,1,0.005), p_a= seq(0,1,0.005))
m0$p_f <- apply(m0, 1, get_pf, xi=x0, mu=0.3, n_e=1000, n_t=514)
m0 <- m0[m0$p_f <= 1,]

m1 <- expand.grid(p_r = seq(0,1,0.005), p_a= seq(0,1,0.005))
m1$p_f <- apply(m1, 1, get_pf, xi=x1, mu=0.3, n_e=1000, n_t=514)
m1 <- m1[m1$p_f <= 1,]

cs <- c(0.3, 0.8, 0.7)

alphas <- get_oc_pc(cs, n=30, m=m0, tI=T)
betas <- get_oc_pc(cs, n=30, m=m1, tI=F)

c(-min(alphas), 1-min(betas))
```

We can search over all possible progression criteria and plot the best available error rates.

```{r}
cs <- sobol(10000, 3)

alphas <- apply(cs, 1, get_oc_pc, n=30, m=m0, tI=T)
df <- cbind(cs, -apply(alphas, 2, min))
betas <- apply(cs, 1, get_oc_pc, n=30, m=m1, tI=F)
df <- as.data.frame(cbind(df, 1-apply(betas, 2, min)))
names(df) <- c("c_r", "c_a", "c_f", "a", "b")
# saveRDS(ocs, "./data/sobol_pc.Rda")

opt <- as.data.frame(paretoFilter(as.matrix(df[,4:5])))
names(opt) <- c("a", "b")
# saveRDS(opt, "./data/opt_pc.Rda")
```

```{r}
df <- readRDS("./data/sobol_pc.Rda")
opt <- readRDS("./data/opt_pc.Rda")

ggplot(df, aes(a,b)) + geom_point(alpha=0.2) +
  geom_point(data=opt, colour=cols[1]) +
  theme_minimal() + coord_fixed()
```

We can search over all possible progression criteria and plot the best available error rates. As a check that the optimisation algorithm is working well, we also evaluate a $10^4$ size space-filling design (specifically, a Sobol sequence) and plot the results.


```{r, eval=F}
opt <- nsga2(get_ocs_pc, 3, 2, n=30, x0=x0, x1=x1, n_e=1000, n_t=514,
             lower.bounds = c(0, 0, 0), upper.bounds = c(1, 1, 1),
              popsize = 100, generations = 100)
# saveRDS(opt, "./data/opt_pc.Rda")

to_eval <- sobol(1000,3)
ocs <- apply(to_eval, 1, get_ocs_pc, n=30, x0=x0, x1=x1, n_e=1000, n_t=514)
# saveRDS(ocs, "./data/sobol_pc.Rda")
```

```{r}
# Optimisation
opt <- readRDS("./data/opt_pc.Rda")
opt_df <- as.data.frame(opt$value)

# Sobol sequence
ocs <- readRDS("./data/sobol_pc.Rda")
sob_df <- as.data.frame(t(ocs))

names(opt_df) <- names(sob_df) <- c("a", "b")
opt_df$t <- 1; sob_df$t <- 2
df <- rbind(opt_df, sob_df)

p <- ggplot(df, aes(a,b)) + geom_point() + theme_minimal() +
  transition_states(t)
animate(p, nframes = 2, fps=0.8)
```
```{r}
df <- cbind(sobol(1000,3), t(ocs))

sub <- df[df[,4]>0.8 & df[,5]>0.8, ]
sub[which.min(0.5*sub[,4] + 0.5*sub[,5]),]

sub2 <- df[df[,4]>0.9 & df[,5]>0.9, ]
sub2[which.min(0.5*sub2[,4] + 0.5*sub2[,5]),]

get_ocs_pc(c(0.3164062, 0.8085938, 0.7460938), n=50, x0, x1, n_e, n_t)
get_ocs_pc(c(0.2832031, 0.7832031, 0.7519531), n=50, x0, x1, n_e, n_t)
```


## Evaluation

We find the OC curves produced by the proposed method and for the comparator for a range of null hypothesis power parameters and a range of total sample sizes of the main trial. FOr each scenario, we consider pilot sample sizes of n = 30, 50 and 100.

```{r, eval = F}
eval_scenario <- function(null_pow, n_t, n)
{
  print(n)
  p0 <- null_pow; p1 <- 0.8

  x0 <- qnorm(p0) + qnorm(0.975)
  x1 <- qnorm(p1) + qnorm(0.975)
  
  opt <- nsga2(fn=get_ocs, 7, 2, 
             n=n, x0=x0, x1=x1, mu=0.3, n_e=1000, n_t=n_t,
             lower.bounds = c(2, rep(0.1,6)), upper.bounds = c(3, rep(1,6)),
             generations = 200, popsize = 100,
             constraints = get_constr, cdim=2)
  
  opt_comp <- nsga2(get_ocs_pc, 3, 2, n=n, x0=x0, x1=x1, n_e=1000, n_t=n_t,
             lower.bounds = c(0, 0, 0), upper.bounds = c(1, 1, 1),
              popsize = 100, generations = 200)
  
  df <- data.frame(a=c(opt$value[,1], opt_comp$value[,1]),
                   b=c(opt$value[,2], opt_comp$value[,2]),
                   t=c(rep("FT", 100), rep("PC", 100)))
  df$n <- n; df$n_t <- n_t; df$null_pow <- null_pow
  
  df <- cbind(df, rbind(cbind(opt$par[,1], rep(0,100), rep(0,100)), opt_comp$par))
  names(df)[7:9] <- c("c_1", "c_2", "c_3")
  
  return(df)
}

df <- NULL
for(i in c(468, 514, 562)){
  for(j in c(0.6, 0.65, 0.7)){
    for(k in c(30, 50, 100)){
          df <- rbind(df, eval_scenario(j, i, k))
    }
  }
}

saveRDS(df, "./data/eval.Rda")
```

Plot the results

```{r}
df <- readRDS("./data/eval.Rda")
names(df)[4] <- "np"
df$a <- ifelse(df$t=="FT", -df$a, df$a)
df$b <- ifelse(df$t=="FT", -df$b, df$b)

labels_nt <- c("468" = "468",  "514" = "514", "562" = "562")

ggplot(df[df$t == "FT" | df$np == 50,], aes(a, b, colour=as.factor(np), linetype=t)) + geom_line() + 
  facet_grid(n_t ~ null_pow, labeller = labeller(n_t = labels_nt)) +
  theme_minimal() + coord_fixed() +
  xlab(expression(alpha)) + ylab(expression(beta)) +
  scale_color_manual(name=expression(n[p]), values=cols) +
  scale_linetype(name="Method", labels=c("Feasibility test", "Standard practice")) +
  theme(panel.spacing = unit(1, "lines"), legend.position="bottom") +
  scale_x_continuous(breaks=c(0, 0.25, 0.5, 0.75, 1)) 

ggsave("./paper/figures/eval.pdf", height=16, width=18, units="cm")
ggsave("./paper/figures/eval.eps", height=16, width=18, units="cm", device = cairo_ps())
```

Pick out some specific numbers for comparison.
```{r}
df[df$n_t == 234 & df$a > 0.08 & df$a < 0.12 & df$t == "FT" & df$np == 30,]
```

Look at one scenario in more detail.
```{r}
sub <- df[df$null_pow == 0.65 & df$n_t == 514,]

ggplot(sub, aes(a,b,colour=as.factor(np), linetype=t)) + geom_step() + #geom_point() +
  theme_minimal() + coord_fixed() +
  xlab(expression(alpha)) + ylab(expression(beta)) +
  scale_color_manual(name=expression(n[p]), values=cols) +
  scale_linetype(name="Method", labels=c("Feasibility test", "Standard practice")) +
  scale_x_continuous(breaks=c(0, 0.2, 0.4, 0.6, 0.8, 1)) + 
  scale_y_continuous(breaks=c(0, 0.2, 0.4, 0.6, 0.8, 1)) 

ggsave("./paper/figures/ex_ocs.pdf", height=9, width=14, units="cm")
ggsave("./paper/figures/ex_ocs.eps", height=9, width=14, units="cm", device = cairo_ps())
```

And look at the corresponding solution spaces:
```{r}
ggplot(sub[101:200,], aes(c_1, c_2, colour=c_3)) + geom_point() +
  theme_minimal() + xlab(expression(c[1])) + ylab(expression(c[3])) +
  labs(color = expression(c[3]))
```

We can relax the assumption of independance between the follow-up and adherence outcomes. We see that as the odds ratio increases, power decreases (this relationship is also noted in Bryant and Day). We might resonably restrict attention to postive odds ratios - we wouldn't expect adherence and follow-up to be inversly correlated. Then we should define type I error with an OR of 1, and type II with an OR of inf. This is very conservative - better to recommend a sensitivity analysis around some (possibly conservative) guess of the true OR?

We use the notation $p_{11}$ to denote the probability a participant will both adhere and be followed up; $p_{10}$ to denote adherence but loss to follow-up; and similarly for $p_{01}$ and $p_{00}$. We specify the model in terms of the conditional probability of adherence given follow-up, $\phi_a = p_{11}/(p_{11} + p_{01})$; overall probability of follow-up, $\phi_f = p_{01} + p_{11}$; and the odds ratio $\phi_{OR} = p_{00}p_{11}/p_{01}p_{10}$. These inputs translate into the following probabilities:
$$
\begin{aligned}
p_{11} &= \phi_a \phi_f \\
p_{01} &= \phi_f - p_{11} \\
p_{00} &= \frac{\phi_{OR} ~ p_{01}(1-p_{11}-p_{01})}{p_{11} + \phi_{OR} ~ p_{01}} \\
p_{10} &= 1 - p_{11} - p_{01} - p_{00}.
\end{aligned}
$$


```{r}
get_oc_cor <- function(phi, n, cc, xi, a=TRUE, mu=0.3, n_e, n_t, or)
{
  p_r <- phi[1]; p_a <- phi[2]; sd <- 1
  if(any(c(p_r,p_a) < 0) | any(c(p_r,p_a) > 1) ){return(1000)}
  exp_n <- get_exp_N(n_e, n_t, p_r)
  p_f <- (2*sd^2 + mu^2 *p_a*(1-p_a))*xi^2/((p_a*mu)^2*exp_n)
  if(any(c(p_r,p_a,p_f) < 0) | any(c(p_r,p_a,p_f) > 1) ){
    1000
  } else {
    (a*-1 + !a*1)*get_comp_ocs_cpp4(cc, n, matrix(c(p_r, p_a, p_f, or), ncol=4), mu, n_e, n_t)
  }
}

get_ocs_cor <- function(c, n, x0, x1, mu=0.3, n_e, n_t, or)
{
  opt_a <- optim(c(0.9,0.9), fn=get_oc_cor,
                 cc=c, n=n, xi=x0, a=T, mu=mu, n_e=n_e, n_t=n_t, or=1)
  
  opt_b <- optim(c(0.9,0.9), fn=get_oc_cor,
                 cc=c, n=n, xi=x1, a=F, mu=mu, n_e=n_e, n_t=n_t, or=or)
  
  c(-opt_a$value, 1-opt_b$value)
}

get_ocs_cor(2.6, n=50, x0=qnorm(0.65) + qnorm(0.975), x1, mu=0.3, n_e=1000, n_t=514, or=5)
```

## Extensions

### Estimating outcome SD

We have a c++ function implementing the relevant equation. We can check it against a simulation:

```{r}
# Compare with simulation
sim <- function(phi, n, c, xi, mu, n_e, n_t)
{
  p_r <- phi[1]; p_a <- phi[2]; p_f <- phi[3] 
  exp_n <- get_exp_N(n_e, n_t, p_r)
  sd <- sqrt( (p_f*(p_a*mu)^2*exp_n - 2*mu^2 *p_a*(1-p_a)*xi^2)/(4*xi^2) )
  
  a_est <- rbinom(1, n, p_a)/n
  f <- rbinom(1, 2*n, p_f)
  f_est <- f/(2*n)
  r_est <- 2*n/(rnbinom(1, 2*n, p_r) + 2*n)
  var_est <- rchisq(1, f-1)*(sd^2)/(f-1)
  
  #var_crit <- (mu*mu*a_est*a_est*f_est*get_exp_N(n_e, n_t, r_est) - 2*mu*mu*a_est*(1-a_est)*c*c)/(4*c*c)
  #pchisq(var_crit*(f-1)/(sd^2), f-1)
  
  stat <- mu*a_est*sqrt(get_exp_N(n_e, n_t, r_est)*f_est)/sqrt((4*var_est + 2*mu*mu*a_est*(1-a_est)))
  stat > c
}
  
phi <- c(0.5196304, 0.6670084, 0.9997755); n_e <- 1000; n_t <- 514; xi <- x1; mu <- 0.3
p_r <- phi[1]; p_a <- phi[2]; p_f <- phi[3] 
exp_n <- get_exp_N(n_e, n_t, p_r)
sd <- sqrt( (p_f*(p_a*mu)^2*exp_n - 2*mu^2 *p_a*(1-p_a)*xi^2)/(4*xi^2) )

get_ocs_var_cpp(c=2.4, n=30, matrix(c(phi, sd), ncol=4), mu=0.3, n_e=1000, n_t=514)
mean(replicate(10000, sim(phi, n=30, c=2.4, xi=x1, mu=0.3, n_e=1000, n_t=514)))
```

Now we can find error rates under the same multi-objective optimisation framework as before:

```{r}
get_ocs_var <- function(y, n, x0, x1, mu=0.3, n_e, n_t)
{
  # Input y = (c, [point in null space], point in alternative space])
  c <- y[1]
  
  # Probability of +ve result at the null point
  p_r <- y[2]; p_a <- y[3]; p_f <- y[4]; sd <- y[5]
  alpha <- get_ocs_var_cpp(c=c, n=n, matrix(c(p_r, p_a, p_f, sd), ncol=4), mu, n_e, n_t)

  # Probability of -ve result at the alternative point
  p_r <- y[6]; p_a <- y[7]; p_f <- y[8]; sd <- y[9]
  beta <- 1 - get_ocs_var_cpp(c=c, n=n, matrix(c(p_r, p_a, p_f, sd), ncol=4), mu, n_e, n_t)

  # Returning negative values for optimisation later
  return(-c(alpha, beta))
}

# For example,
get_ocs(c(2.4, 0.64, 0.73, 0.8769883, 1, 0.96, 1, 0.7422108, 0.9), 30, x0, x1, 0.3, 1000, 514) 
```

```{r}
get_constr_var <- function(y, n, x0, x1, mu=0.3, n_e, n_t)
{
  p_r <- y[2]; p_a <- y[3]; p_f <- y[4]; sd <- y[5]
  exp_n <- get_exp_N(n_e, n_t, p_r)
  con_a <-  p_a*mu*sqrt(p_f*exp_n)/sqrt(4*sd^2 + 2*mu^2 * p_a*(1-p_a))

  p_r <- y[6]; p_a <- y[7]; p_f <- y[8]; sd <- y[9]
  exp_n <- get_exp_N(n_e, n_t, p_r)
  con_b <-  p_a*mu*sqrt(p_f*exp_n)/sqrt(4*sd^2 + 2*mu^2 * p_a*(1-p_a))

  return(c(x0-con_a, con_b-x1))
}

# For example, change the alternative point from above so it is outside the alternative hypothesis
get_constr_var(c(2.4, 0.64, 0.73, 0.8769883, 1, 0.96, 0.9, 0.7422108, 0.95), 30, x0, x1, 0.3, 1000, 514)
```

Now optimise, but with the search constrained to $0.8 < \sigma$:

```{r, eval = F}
ptm <- proc.time()

opt <- nsga2(fn=get_ocs_var, 9, 2, 
             n=50, x0=qnorm(0.65) + qnorm(0.975), x1=x1, mu=0.3, n_e=1000, n_t=514,
             lower.bounds = c(2, 0.1,0.1,0.1, 0.8,  0.1,0.1,0.1, 0.8),
             upper.bounds = c(3, 1,1,1, 2, 1,1,1, 2),
             generations = 1:200, popsize = 100,
             constraints = get_constr_var, cdim=2)

proc.time() - ptm

#   user  system elapsed 
# 3916.90    5.93 3963.75 , so ~ 1.1 hours 

#saveRDS(opt, "./data/opt_var.Rda")
```

Check convergence:

```{r}
DHs <- NULL
for(i in 1:length(opt)){
  dh <- dominatedHypervolume(-opt[[i]]$value, c(1,1))
  DHs <- c(DHs, dh)
}
plot(DHs)
```
Compare with the same scenario but where we didn't include sd estimation:

```{r}
opt <- readRDS("./data/opt.Rda")
opt_var <- readRDS("./data/opt_var.Rda")

df <- data.frame(a=-opt[[200]]$value[,1], b=-opt[[200]]$value[,2])
df_var <- data.frame(a=-opt_var[[200]]$value[,1], b=-opt_var[[200]]$value[,2])

df <- rbind(df, df_var)
df$t <- c(rep("Known variance", 100), rep("Unknown variance", 100))

ggplot(df, aes(a, b, shape=t, linetype=t)) + 
  geom_point() +
  geom_line() +
  scale_shape_manual(values=c(16, 2)) +
  xlab(expression(alpha)) + ylab(expression(beta)) +
  labs(shape = "", linetype="") +
  theme_minimal() +
  scale_y_continuous(breaks = seq(0,1,0.2)) + scale_x_continuous(breaks = seq(0,1,0.2)) +
  coord_fixed()

ggsave("./paper/figures/var_ocs.pdf", height=9, width=14, units="cm")
ggsave("./paper/figures/var_ocs.eps", height=9, width=14, units="cm", device = cairo_ps())
```

We see empirically that error rates are maximised when $\sigma$ is minimised. So, firstly, we need to choose some lower bound for $\sigma$ that we want to control error rates for. Secondly, can we see analytucally why this is the case, and thus reduce the problem back to one of fixed $\sigma$ (at the chosen lower bound)? As $\sigma$ decreases, one of the three rate parameters will also have to decrease to stay on the hypothesis border. Broadly, error rates will be maximised when the variance of the statistic is maximised. As $\sgima$ decreases, the variance of the statistic will increase (via increase in the $1/\sigma$ term). Decresing a rate parameter (whilst being above 0.5) will increase the sample variance and therefore also the statistics variance. We can only show this all empirically at the moment.

Note that there is a natural _upper_ limit for the variance when we compute our hypotheses - as the variance increases (gets worse), the other three parameters have to increase (get better) to compensate, but they can't exceed 1. So we can stick any upper limit into our optimiser. But the lower limit is more sensitive - as variance decreases, we allow much lower rates into our hypotheses, and this leads to increases in the error rates. So we need to choose a variance, below which we are not interested in controlling errors. 

### Using the pilot estimate

So far, we have assumed that the size of the main trial has been fixed in advance, and defined our hypotheses based on the power that the fixed size will obtain. After the pilot we will have an estimate - also our test sttaistic - of the inflated sample size the main trial will need to obtain 80\% power, so it might be reasonable to use this for our main trial. In particular, we might restrict ourselves to inflations of the original suggestion. Togther with the test ruling out results below the critical value, we are then controlling the probability of being in segments of the power curves corresponding to our null and alternatives. 

```{r}
get_pow <- function(n, pr)
{
  pnorm(0.3*pr[3]*sqrt(n*pr[1]*pr[2])/sqrt(2*pr[4]^2 + 0.3^2 * pr[3]*(1-pr[3])) - qnorm(0.975))
}

pr <- as.numeric(null[100,])

# Check against simulation
f <- function(n_i, pr)
{
  ns <- rbinom(2, round(n_i), pr[1]*pr[2]); 
  a <- rbinom(1, ns[2], pr[3])
  
  y_1 <- c(rnorm(a, 0.3, pr[4]), rnorm(ns[2]-a, 0, pr[4]))
  y_0 <- rnorm(ns[1], 0, pr[4])
  return(t.test(y_1, y_0, alternative = "greater", conf.level = 0.975)$p.value < 0.025)
}

get_pow(n_i, pr)
mean(replicate(10000, f(n_i, pr)))
```
How variable are the power functions in each of our hypotheses sample sets? Pick a few at random and plot:
```{r}
ns <- seq(10,700,20)
df <- NULL
for(i in 1:200){
  pr <- as.numeric(null[i,])
  pows <- sapply(ns, function(x) get_pow(x, pr))
  df <- rbind(df, cbind(ns, pows, r=rep(i, length(ns))))
}

ggplot(as.data.frame(df), aes(ns, pows, group=r)) + geom_line(alpha=0.1) +
  geom_hline(yintercept = 0.5, linetype=2) + geom_vline(xintercept = 174, linetype=2) + 
  theme_minimal()
```

This variability is coming from the optimisation not always being exact. Recall that we defined the null as those points in the parameter space such that we get a power of 0.5 with $n^*$. But if we have two points $\phi_0, \phi_1$ which both do this, they will give equal power for all other $n$. This is clear if we re-write the power for the main trial as
$$
\Phi \left( \sqrt{n^*} x - z_{1-\alpha} \right),
$$
where
$$
x = \frac{ \phi_a\mu \sqrt{\phi_r \phi_f} } {\sqrt{2\sigma^2 + \mu^2 \phi_a(1-\phi_a)}}.
$$
So, no matter how it is composed, it is the facotr $x$ which determines the power curve. So we can plot our exact hypotheses in terms of the power curves. For $n^* = 174$, 
```{r}
ns <- seq(10,700,2)

x0 <- (qnorm(0.6) + qnorm(0.975))/sqrt(235)
x1 <- (qnorm(0.8) + qnorm(0.975))/sqrt(235)

p0s <- sapply(ns, function(n) pnorm(sqrt(n)*x0 - qnorm(0.975)))
p1s <- sapply(ns, function(n) pnorm(sqrt(n)*x1 - qnorm(0.975)))

df <- data.frame(n=rep(ns,2), p=c(p0s, p1s), h=c(rep("null", length(ns)), rep("alt", length(ns))))

ggplot(df, aes(n, p, colour=h)) + geom_line() + geom_vline(xintercept = 174, linetype=2) +
  theme_minimal()
```
Now, if we allow the sample size of the main trial to be chosen as a result of the pilot, we can think of our test not as excluding trials which will have 50\% power or less while maintaining those with 80\% power or more; rather, we are excluding trials which will get us somehwere on the null power curve or worse, etc. If we say that the main sample should not be less than $n^*$ even if the pilot sample says that's what is needed, then we can narrow it down to some sections on these curves.

```{r}
lo <- 174; hi <- 300 # hi is the critical value used in our test
sub <- df[df$n >= lo & df$n <= hi, ]

ggplot(df, aes(n, p, colour=h)) + geom_line() + geom_vline(xintercept = 174, linetype=2) +
  geom_line(data=sub, size = 2) +
  theme_minimal()
```

### Assessing efficacy

If we would like to simultaneously assess the effectiveness of the intervention alongside the feasibility of the trial, we must reconsider how we define our hypotheses. One way to approach this is to define a value function over the two dimensional space of effectiveness and power of the main trial (assuming the sample size is fixed). If we can say that value in one dimension should be linear when the other attribute is fixed at some level, we get a value function
$$
v(\beta, \phi_a \mu) = \frac{(1-\beta) \phi_a \mu}{\mu^*},
$$
where $\mu^*$ is some (arbitrary) maximum value. This will give a value of 1 at the point $(0, \mu^*)$, and 0 whenever $\beta=1$ or $\mu=0$.

```{r}
# A value function for power and effectiveness
get_value <- function(x)
{
  pow <- x[1]; eff <- x[2]
  eff_max <- 1
  # End value for maximal eff is just pow
  # value is linear going from pow = 0 to pow = 1
  v <- ((eff+0.3)/eff_max)*pow
  
  # true MCID is about 0.18 - 0 value for all powers here
  # grad of v as a function of mu determined by fixed power
  m <- pow*2-1
  c <- -m*0.18
  v <- (c+m*eff)*eff
  return(v)
}

df <- expand.grid(pow=seq(0,1,0.05), eff=seq(0,1,0.05))
df$v <- apply(df, 1, get_value)

v_n <- get_value(c(0.3, 0.3)) # null value
v_a <- get_value(c(0.9, 0.3)) # alternative value
df$h <- ifelse(df$v < v_n, "N", ifelse(df$v > v_a, "A", "-"))

ggplot() + geom_contour(data=df, aes(eff, pow, z=v)) + coord_fixed() +
  geom_point(data=df, aes(eff, pow, colour=h), alpha=0.2)
  

  #geom_point(data=data.frame(eff=c(0.4264519, 0.4826196), pow=c(0.415072, 0.4984145)), aes(eff,pow))
```

[Note - we are treating power as a deterministic quantity here, hence using a value function (which encodes preferences under certainty) as opposed to a utility function (which encodes preferences under uncertainty). But if we extend the model slightly, we can view power as a simple gamble between a 0 treatment improvement (when we fail to reject the null and stick with standard care) and a $\mu$ treatment improvement. If we were to use the same function to describe our preferences, this would imply we are risk-nuetral with regards to the level of treatment imporvement we will obtain after the trial. This may be rather unlikely, particularly from the perspective of an individual who stands to benefit from such an improvement.]

With this value function and these thresholds for the null and alternative hypotheses, we can now approximate these again:
```{r}
obj_f_null2 <- function(x, n_i)
{
  p_r <- x[1]; p_f <- x[2]; p_a <- x[3]; mu <- x[4]
  eff_n <- n_i*(p_r*p_f*p_a^2)
  eff <- mu*p_a
  pow <- 0
  if(eff_n>1) pow <- power.t.test(n=eff_n, delta=mu, sd=1)$power
  v <- get_value(c(pow, eff))
  pen <- 100000*(v > 0.3)
  -x + pen
}

obj_f_alt2 <- function(x, n_i)
{
  p_r <- x[1]; p_f <- x[2]; p_a <- x[3]; mu <- x[4]
  eff_n <- n_i*(p_r*p_f*p_a^2)
  eff <- mu*p_a
  pow <- 0
  if(eff_n>1) pow <- power.t.test(n=eff_n, delta=eff, sd=1)$power
  v <- get_value(c(pow, eff))
  pen <- 100000*(v < 0.48)
  x + pen
}

opt <- nsga2(obj_f_null2, 4, 4, n_i=n_i,
             lower.bounds = c(0, 0, 0, 0), upper.bounds = c(1, 1, 1, 0.5),
             popsize = 1000)
null <- as.data.frame(-opt$value)
names(null) <- c("p_r", "p_f", "p_a", "mu")
plot(null)

opt <- nsga2(obj_f_alt2, 4, 4, n_i=n_i,
             lower.bounds = c(0, 0, 0, 0), upper.bounds = c(1, 1, 1, 0.5),
             popsize = 1000)
alt <- as.data.frame(opt$value)
names(alt) <- c("p_r", "p_f", "p_a", "mu")
plot(alt)
```

Given these hypotheses, we can extend our test statistic to
$$
T = S \times \hat{\mu}.
$$
The power function can be derived by a simple extension of the above formula. There, we had a term $Pr\bigg[ \hat{\phi_r}\frac{f}{2n_p}\frac{a^2}{n^2} > c \mid f, a, \phi \bigg]$, wheras now we must evaluate 
$$
Pr\bigg[ \hat{\mu}\hat{\phi}_a\hat{\phi_r}\frac{f}{2n_p}\frac{a^2}{n^2} > c \mid f, a, \phi, \mu \bigg] =
Pr\bigg[ \frac{\hat{\mu} f a^3}{(r_- + 2n_p)n_p^3} > c \mid f, a, \phi, \mu \bigg].
$$
The condition reduces to
$$
r_- < \frac{\hat{\mu}fa^3 - 2n_p^4c}{cn_p^3}
$$
Thus, 
$$
\begin{aligned}
Pr \left[ r_- < \frac{\hat{\mu}fa^3 - 2n_p^4c}{cn_p^3} \mid f, a, \phi, \mu \right] &= 
\int Pr \left[ r_- < \frac{\hat{\mu}fa^3 - 2n_p^4c}{cn_p^3} \mid \hat{\mu}, f, a, \phi, \mu \right] f(\hat{\mu} \mid \phi, \mu) d\hat{\mu} \\
&= \int F_{r_-}\left(\frac{\hat{\mu}fa^3 - 2n_p^4c}{cn_p^3} \mid \hat{\mu}, f, a, \phi, \mu \right) f(\hat{\mu} \mid \phi, \mu) d\hat{\mu}.
\end{aligned}
$$
Assymptotically, the approximate sampling distribution for the mean effect ITT complete case estimate depends on the numbers of adherers who are successfully followed up. That is, we need more than just the number of adherers and the number of successful follow-ups. If we denote by $f_0, f_1$ the numbers followed up in each arm, and by $a_{1,f}$ the number in the intervention arm who both adhere and are followed-up, then

$$
f(\hat{\mu} \mid a_{1,f}, f_0, f_1, \phi, \mu) \approx N\left(\frac{a_{1,f}}{f_1}\mu, \frac{\sigma^2}{f_0 + f_1} \right).
$$
This presents a computational challange compared with the previous case where we didn't look at efficacy, becasue now we must enumerate over all possible ways to allocate the participants into the two-by-two table of adhering and following up. For moderate sample sizes this will lead to a huge number of cases, each of which requiring the above integral to be solved. So, to keep things tractable, we will use an MC approximation.

The code below simulates $N$ trial outcomes. We first simulate the adherence and follow-up in the intervention group using a multinomial distribution, where we have assumed that these outcomes are independant (i.e. knowing a participant has adhered tells us nothing about whether or not the will be followed up). Then we simulate follow-up in the control arm, and the number of failed recruitment attampts needed to reach the target sample size of $2n$. Finally we simulate the observed mean difference, then calculate the statistic $T$ and compare with the threshold value. Note that to calculate the observed differences we first simulate $N$ standard normal deviates, and then translate these into the required distribution, recalling that

$$
X \sim N(0,1) \rightarrow \sqrt{y}X + z \sim N\left(z , y^2\right).
$$
Once the data are simulated we do the tranformations and computations in c++ to keep things relatively fast.

```{r}
get_comp_ocs_eff_MC <- function(rule, n, par, sigma, N)
{
  # par = (phi_r, phi_f, phi_a, mu)
  
  # Simulate outcomes in the intervention arm
  # (af, a!f, !af, !a!f)
  m <- t(rmultinom(N, n, c(par[3]*par[2], par[3]*(1-par[2]), (1-par[3])*par[2], (1-par[3])*(1-par[2]))))
  
  # Simulate follow-up in the control arm
  f0 <- rbinom(N, n, par[2])
  
  # Simulate number of failures before recruitment was met
  s <- rnbinom(N, 2*n, par[1])
  
  df <- cbind(m,f0,s)
  
  ns <- rnorm(N)
  
  ts <- MC_cpp(df, ns, sigma, par[4], n)
  ts <- cbind(ts, (ts[,1]+0.3)*ts[,2])
  #hist(ts[,3])
  ts[is.na(ts[,5]),5] <- 0
  
  alpha <- mean(ts[,5] > rule)
  return(alpha)
}

#df <- as.data.frame(ts[1:1000,])
#df$V5 <- df$V1*df$V2

#ggplot(df, aes(V1, V2, colour=V4)) + geom_point() + coord_fixed()
```

Now getting the maxmimum and minimum error rates:
```{r}
get_max_comp_ocs_eff <- function(rule, n, sigma, null, alt)
{
  N <- 1000
  # get max alpha
  a <- max(apply(null, 1, function(x, rule, n, sigma) get_comp_ocs_eff_MC(rule, n, x, sigma, N), rule=rule, n=n, sigma=sigma))

  # get max beta
  b <- 1 - min(apply(alt, 1, function(x, rule, n, sigma) get_comp_ocs_eff_MC(rule, n, x, sigma, N), rule=rule, n=n, sigma=sigma))
  
  return(c(a,b))
}

# For example
get_max_comp_ocs_eff(rule=0.5, n=100, sigma=1, null[1:100,], alt[1:100,])
```

What kind or error rates are possible as we alter the progression criteria?

```{r, eval=F}
cs <- seq(0.1, 0.6, 0.05)
df <- as.data.frame(cbind(cs, t(sapply(cs, get_max_comp_ocs_eff, n=100, sigma=1, null=null, alt=alt))))
names(df)[2:3] <- c("a", "b")

ggplot(df, aes(a,b)) + geom_point() + coord_fixed() + geom_abline(intercept = 1, slope = -1)
```

## References

`r knitr::knit_exit()`



We can plot the surface we are optimising over and see where the maximum error rates are obtained, say for $n=30$ and $c=2.5$:

```{r}
# Type I
df <- expand.grid(p_r=seq(0,1,0.01), p_a=seq(0,1,0.01))
df$p_f <- apply(df, 1, get_fup, xi=x0, mu=mu, n_e=235, n_t=235)
sub <- df[df$p_f <= 1 & df$p_r <=1,]
sub$a <- -apply(sub, 1, get_oc, n=30, cc=2.4, xi=x0, n_e=235, n_t=235)
max(sub$a)

#ptm <- proc.time()
#x <- replicate(500, get_oc(as.numeric(sub[10,1:2]), n=30, cc=2.4, xi=x0, n_e=800, n_t=235))
#proc.time() - ptm

ggplot(sub, aes(p_r, p_a, z=a)) + geom_contour(aes(colour=..level..)) +
  theme_minimal() + xlab("R") + ylab("A") + 
  guides(colour = guide_legend(title = expression()))

# Type II
df <- expand.grid(p_r=seq(0,1,0.03), p_a=seq(0,1,0.03))
df$p_f <- apply(df, 1, get_fup, xi=x1, mu=mu, n_e=800, n_t=235)
sub <- df[df$p_f <= 1 & df$p_r <= 1,]
sub$a <-  -apply(sub, 1, get_oc, n=30, cc=2.4, xi=x1, n_e=800, n_t=235)
min(sub$a)

ggplot(sub, aes(p_r, p_a, z=a)) + geom_contour(aes(colour=..level..)) +
  theme_minimal() + xlab("R") + ylab("Adherence") + 
  guides(colour = guide_legend(title = expression()))
```

We see that error rates appear to be maximised when $p_r = p_f = 1$ and $p_a$ is minimised. But this is not actually the case. Take the point (0.9968275,1,0.685), which gives $\alpha = 0.225$. In contrast, the point (0.9543184,1,0.7) gives a higher $\alpha = 0.261$. On the other hand, type II error does seem to be consistently maximised when $p_r = p_f = 1$.


